"""Service principal de scraping orchestrant tous les composants avec cache."""

import asyncio
from datetime import datetime
from typing import Any, Optional
import hashlib

from ..config import get_logger
from ..llm import generate_with_ollama
from ..models.enums import OutputFormat, TaskStatus
from ..models.schemas import ContentExtraction, ScrapingTaskCreate
from ..cache import get_cache_manager
from ..cache.models import generate_cache_key, CacheLevel
from .browser import PageScraper, browser_manager
from .extractor import content_extractor

logger = get_logger("scraping.service")


class ScrapingService:
    """Service principal de scraping avec cache multi-niveau."""

    def __init__(self):
        self.logger = logger
        self.page_scraper = PageScraper(browser_manager)
        self.content_extractor = content_extractor
        self._active_tasks: dict[str, asyncio.Task] = {}
        self.cache_manager = None  # Sera initialis√© lors du premier usage

    async def _ensure_cache_manager(self):
        """Assure que le gestionnaire de cache est initialis√©."""
        if self.cache_manager is None:
            self.cache_manager = await get_cache_manager()
    
    def _generate_cache_key(self, task_data: ScrapingTaskCreate) -> str:
        """G√©n√®re une cl√© de cache pour une t√¢che de scraping."""
        cache_params = {
            "output_format": task_data.output_format.value,
            "llm_provider": task_data.llm_provider.value if task_data.llm_provider else None,
            "use_llm": getattr(task_data, 'use_llm', True),
            "custom_instructions": getattr(task_data, 'custom_instructions', None),
        }
        return generate_cache_key(str(task_data.url), cache_params)
    
    async def scrape_url(
        self,
        task_data: ScrapingTaskCreate,
        task_id: str = None,
        progress_callback: Optional[callable] = None,
        use_cache: bool = True,
    ) -> dict[str, Any]:
        """
        Scrape une URL compl√®tement.

        Args:
            task_data: Donn√©es de la t√¢che de scraping
            task_id: ID de la t√¢che pour le suivi
            progress_callback: Callback pour reporter le progr√®s

        Returns:
            Dict contenant tous les r√©sultats du scraping
        """
        start_time = datetime.utcnow()
        url = str(task_data.url)

        try:
            self.logger.info(f"üöÄ D√©but du scraping complet: {url}")

            if progress_callback:
                await progress_callback(task_id, 10, "Initialisation du navigateur")

            # √âtape 1: Scraping de la page avec Playwright
            self.logger.debug("üìÑ √âtape 1: R√©cup√©ration de la page")
            page_data = await self.page_scraper.fetch_page(url)

            if page_data.get("error"):
                raise Exception(f"Erreur de navigation: {page_data['error']}")

            if not page_data.get("html"):
                raise Exception("Aucun contenu HTML r√©cup√©r√©")

            if progress_callback:
                await progress_callback(
                    task_id, 40, "Page r√©cup√©r√©e, extraction du contenu"
                )

            # √âtape 2: Extraction du contenu principal
            self.logger.debug("üîç √âtape 2: Extraction du contenu principal")
            content_extraction = self.content_extractor.extract_main_content(
                page_data["html"], url
            )

            if progress_callback:
                await progress_callback(task_id, 70, "Contenu extrait, formatage final")

            # √âtape 3: Extraction des donn√©es structur√©es
            self.logger.debug("üìä √âtape 3: Extraction des donn√©es structur√©es")
            structured_data = self.content_extractor.extract_structured_data(
                page_data["html"]
            )

            # √âtape 4: Structuration intelligente avec LLM (si n√©cessaire)
            if (
                task_data.llm_provider
                and task_data.output_format == OutputFormat.MARKDOWN
            ):
                self.logger.debug("üß† √âtape 4a: Structuration intelligente avec LLM")
                try:
                    structured_by_llm = await self._enhance_with_llm(
                        content_extraction.content, task_data.output_format
                    )
                    content_extraction.content = structured_by_llm
                    if progress_callback:
                        await progress_callback(
                            task_id, 80, "Structuration LLM termin√©e"
                        )
                except Exception as e:
                    self.logger.warning(
                        f"Structuration LLM √©chou√©e, utilisation du contenu original: {e}"
                    )

            # √âtape 5: Formatage selon le format demand√©
            self.logger.debug(f"üìù √âtape 5: Formatage en {task_data.output_format}")
            formatted_content = await self._format_content(
                content_extraction, page_data, task_data.output_format
            )

            if progress_callback:
                await progress_callback(task_id, 95, "Finalisation des r√©sultats")

            # Calculer les m√©triques finales
            end_time = datetime.utcnow()
            total_execution_time = int((end_time - start_time).total_seconds() * 1000)

            # Assembler le r√©sultat final
            result = {
                "status": TaskStatus.COMPLETED,
                "url": url,
                "output_format": task_data.output_format,
                "structured_content": formatted_content,
                "raw_content": page_data["html"],
                "task_metadata": {
                    "title": content_extraction.title,
                    "author": content_extraction.author,
                    "word_count": content_extraction.word_count,
                    "reading_time_minutes": content_extraction.reading_time_minutes,
                    "language": content_extraction.language,
                    "tags": content_extraction.tags,
                    "page_metadata": {
                        "status_code": page_data.get("status_code"),
                        "final_url": url,
                        "page_title": page_data.get("title"),
                        "meta_description": page_data.get("meta", {}).get(
                            "description"
                        ),
                        "links_count": len(page_data.get("links", [])),
                        "images_count": len(page_data.get("images", [])),
                    },
                    "structured_data": structured_data,
                    "scraping_timestamp": start_time.isoformat(),
                    "completion_timestamp": end_time.isoformat(),
                },
                "execution_time_ms": total_execution_time,
                "content_size_bytes": len(page_data["html"])
                if page_data["html"]
                else 0,
                "tokens_used": self._estimate_tokens_used(formatted_content),
                "error_message": None,
                "error_details": None,
            }

            if progress_callback:
                await progress_callback(task_id, 100, "Scraping termin√© avec succ√®s")

            self.logger.info(
                f"‚úÖ Scraping termin√© avec succ√®s: {url} "
                f"({total_execution_time}ms, {content_extraction.word_count} mots)"
            )

            return result

        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"‚ùå Erreur lors du scraping de {url}: {error_msg}")

            if progress_callback:
                await progress_callback(task_id, 100, f"Erreur: {error_msg}")

            # Calculer le temps d'ex√©cution m√™me en cas d'erreur
            end_time = datetime.utcnow()
            total_execution_time = int((end_time - start_time).total_seconds() * 1000)

            return {
                "status": TaskStatus.FAILED,
                "url": url,
                "output_format": task_data.output_format,
                "structured_content": None,
                "raw_content": None,
                "task_metadata": {
                    "scraping_timestamp": start_time.isoformat(),
                    "completion_timestamp": end_time.isoformat(),
                },
                "execution_time_ms": total_execution_time,
                "content_size_bytes": 0,
                "tokens_used": None,
                "error_message": error_msg,
                "error_details": {
                    "error_type": type(e).__name__,
                    "url": url,
                    "timestamp": end_time.isoformat(),
                },
            }

    async def scrape_multiple_urls(
        self,
        urls: list[str],
        output_format: OutputFormat = OutputFormat.MARKDOWN,
        max_concurrent: int = 3,
        progress_callback: Optional[callable] = None,
    ) -> list[dict[str, Any]]:
        """
        Scrape plusieurs URLs en parall√®le.

        Args:
            urls: Liste des URLs √† scraper
            output_format: Format de sortie
            max_concurrent: Nombre maximum de t√¢ches simultan√©es
            progress_callback: Callback pour le progr√®s global

        Returns:
            Liste des r√©sultats de scraping
        """
        self.logger.info(
            f"üîÑ Scraping de {len(urls)} URLs en parall√®le (max {max_concurrent})"
        )

        # Cr√©er un semaphore pour limiter la concurrence
        semaphore = asyncio.Semaphore(max_concurrent)
        results = []

        async def scrape_with_semaphore(url: str, index: int):
            async with semaphore:
                task_data = ScrapingTaskCreate(url=url, output_format=output_format)

                result = await self.scrape_url(task_data, task_id=f"batch_{index}")

                if progress_callback:
                    progress = int(((index + 1) / len(urls)) * 100)
                    await progress_callback(
                        "batch_progress",
                        progress,
                        f"Termin√© {index + 1}/{len(urls)} URLs",
                    )

                return result

        # Lancer toutes les t√¢ches
        tasks = [scrape_with_semaphore(url, i) for i, url in enumerate(urls)]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Convertir les exceptions en r√©sultats d'erreur
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    {
                        "status": TaskStatus.FAILED,
                        "url": urls[i],
                        "error_message": str(result),
                        "execution_time_ms": 0,
                    }
                )
            else:
                processed_results.append(result)

        success_count = sum(
            1 for r in processed_results if r["status"] == TaskStatus.COMPLETED
        )
        self.logger.info(
            f"‚úÖ Scraping batch termin√©: {success_count}/{len(urls)} succ√®s"
        )

        return processed_results

    async def _format_content(
        self,
        content_extraction: ContentExtraction,
        page_data: dict[str, Any],
        output_format: OutputFormat,
    ) -> str:
        """
        Formate le contenu selon le format demand√©.

        Args:
            content_extraction: Contenu extrait
            page_data: Donn√©es brutes de la page
            output_format: Format de sortie d√©sir√©

        Returns:
            Contenu format√© selon le format demand√©
        """
        try:
            if output_format == OutputFormat.MARKDOWN:
                return self._format_as_markdown(content_extraction, page_data)

            elif output_format == OutputFormat.JSON:
                import json

                return json.dumps(
                    {
                        "title": content_extraction.title,
                        "content": content_extraction.content,
                        "author": content_extraction.author,
                        "publication_date": content_extraction.publication_date,
                        "tags": content_extraction.tags,
                        "language": content_extraction.language,
                        "word_count": content_extraction.word_count,
                        "reading_time_minutes": content_extraction.reading_time_minutes,
                        "url": page_data.get("url"),
                        "extracted_at": datetime.utcnow().isoformat(),
                    },
                    indent=2,
                    ensure_ascii=False,
                )

            elif output_format == OutputFormat.XML:
                return self._format_as_xml(content_extraction, page_data)

            elif output_format == OutputFormat.CSV:
                return self._format_as_csv(content_extraction, page_data)

            elif output_format == OutputFormat.HTML:
                return self._format_as_html(content_extraction, page_data)

            elif output_format == OutputFormat.PLAIN_TEXT:
                return content_extraction.content

            else:
                # Fallback vers Markdown
                return self._format_as_markdown(content_extraction, page_data)

        except Exception as e:
            self.logger.error(f"Erreur lors du formatage en {output_format}: {e}")
            # Fallback vers le texte brut
            return content_extraction.content

    def _format_as_markdown(
        self, content: ContentExtraction, page_data: dict[str, Any]
    ) -> str:
        """Formate le contenu en Markdown."""
        lines = []

        # Titre principal
        if content.title:
            lines.append(f"# {content.title}")
            lines.append("")

        # M√©tadonn√©es
        if content.author:
            lines.append(f"**Auteur :** {content.author}")

        if content.publication_date:
            lines.append(f"**Date de publication :** {content.publication_date}")

        if content.language:
            lines.append(f"**Langue :** {content.language}")

        if content.tags:
            tags_str = ", ".join(f"`{tag}`" for tag in content.tags)
            lines.append(f"**Tags :** {tags_str}")

        lines.append(
            f"**Mots :** {content.word_count} | **Lecture :** ~{content.reading_time_minutes} min"
        )
        lines.append("")
        lines.append("---")
        lines.append("")

        # Contenu principal
        lines.append(content.content)

        return "\n".join(lines)

    def _format_as_xml(
        self, content: ContentExtraction, page_data: dict[str, Any]
    ) -> str:
        """Formate le contenu en XML."""
        import xml.etree.ElementTree as ET

        root = ET.Element("article")

        # M√©tadonn√©es
        if content.title:
            title_elem = ET.SubElement(root, "title")
            title_elem.text = content.title

        if content.author:
            author_elem = ET.SubElement(root, "author")
            author_elem.text = content.author

        if content.publication_date:
            date_elem = ET.SubElement(root, "publication_date")
            date_elem.text = content.publication_date

        if content.language:
            lang_elem = ET.SubElement(root, "language")
            lang_elem.text = content.language

        # Tags
        if content.tags:
            tags_elem = ET.SubElement(root, "tags")
            for tag in content.tags:
                tag_elem = ET.SubElement(tags_elem, "tag")
                tag_elem.text = tag

        # Statistiques
        stats_elem = ET.SubElement(root, "statistics")
        word_count_elem = ET.SubElement(stats_elem, "word_count")
        word_count_elem.text = str(content.word_count)
        reading_time_elem = ET.SubElement(stats_elem, "reading_time_minutes")
        reading_time_elem.text = str(content.reading_time_minutes)

        # Contenu principal
        content_elem = ET.SubElement(root, "content")
        content_elem.text = content.content

        return ET.tostring(root, encoding="unicode", method="xml")

    def _format_as_csv(
        self, content: ContentExtraction, page_data: dict[str, Any]
    ) -> str:
        """Formate le contenu en CSV."""
        import csv
        import io

        output = io.StringIO()
        writer = csv.writer(output)

        # En-t√™tes
        writer.writerow(
            [
                "title",
                "author",
                "publication_date",
                "language",
                "word_count",
                "reading_time_minutes",
                "tags",
                "content",
            ]
        )

        # Donn√©es
        writer.writerow(
            [
                content.title or "",
                content.author or "",
                content.publication_date or "",
                content.language or "",
                content.word_count,
                content.reading_time_minutes,
                "; ".join(content.tags) if content.tags else "",
                content.content.replace("\n", " ").replace("\r", " "),
            ]
        )

        return output.getvalue()

    def _format_as_html(
        self, content: ContentExtraction, page_data: dict[str, Any]
    ) -> str:
        """Formate le contenu en HTML."""
        lines = ["<!DOCTYPE html>", "<html>", "<head>", "<meta charset='utf-8'>"]

        if content.title:
            lines.append(f"<title>{content.title}</title>")

        lines.extend(["</head>", "<body>"])

        # Titre principal
        if content.title:
            lines.append(f"<h1>{content.title}</h1>")

        # M√©tadonn√©es
        if any([content.author, content.publication_date, content.language]):
            lines.append("<div class='metadata'>")

            if content.author:
                lines.append(f"<p><strong>Auteur :</strong> {content.author}</p>")

            if content.publication_date:
                lines.append(
                    f"<p><strong>Date :</strong> {content.publication_date}</p>"
                )

            if content.language:
                lines.append(f"<p><strong>Langue :</strong> {content.language}</p>")

            lines.append("</div>")

        # Contenu principal
        lines.append("<div class='content'>")
        # Convertir les retours √† la ligne en paragraphes
        paragraphs = content.content.split("\n\n")
        for para in paragraphs:
            if para.strip():
                lines.append(f"<p>{para.strip()}</p>")
        lines.append("</div>")

        lines.extend(["</body>", "</html>"])

        return "\n".join(lines)

    async def _enhance_with_llm(self, content: str, output_format: OutputFormat) -> str:
        """
        Am√©liore le contenu avec structuration LLM.

        Args:
            content: Contenu brut extrait
            output_format: Format de sortie d√©sir√©

        Returns:
            Contenu structur√© par le LLM
        """
        try:
            # Limiter la taille du contenu pour √©viter les timeouts
            max_content_length = 8000  # ~2000 tokens
            if len(content) > max_content_length:
                content = content[:max_content_length] + "..."
                self.logger.debug(
                    f"Contenu tronqu√© √† {max_content_length} caract√®res pour le LLM"
                )

            # Instructions sp√©cifiques selon le format
            if output_format == OutputFormat.MARKDOWN:
                instruction = """Tu es un expert en r√©daction et structuration de contenu web.

Ton r√¥le:
- Analyser le contenu fourni et le restructurer de mani√®re claire et logique
- Cr√©er une structure Markdown bien organis√©e avec des titres, sous-titres appropri√©s
- Pr√©server toutes les informations importantes
- Am√©liorer la lisibilit√© sans d√©naturer le sens
- Utiliser des listes, citations et mise en forme Markdown quand appropri√©

R√®gles:
- Garde un style professionnel et informatif
- Utilise des titres hi√©rarchiques (##, ###, etc.)
- Organise le contenu en sections logiques
- Pr√©serve les liens et r√©f√©rences importantes
- Si le contenu est trop court, d√©veloppe l√©g√®rement en restant factuel"""

            else:
                instruction = """Tu es un expert en extraction et structuration de contenu web.
Analyse le contenu fourni et structure-le de mani√®re claire et lisible.
Pr√©serve les informations importantes et organise le contenu logiquement."""

            # G√©n√©rer avec Ollama
            structured_content = await generate_with_ollama(content, instruction)

            if structured_content and len(structured_content.strip()) > 100:
                self.logger.info(
                    f"‚úÖ Contenu structur√© par LLM: {len(structured_content)} caract√®res"
                )
                return structured_content
            else:
                self.logger.warning(
                    "R√©ponse LLM trop courte, utilisation du contenu original"
                )
                return content

        except Exception as e:
            self.logger.error(f"Erreur lors de la structuration LLM: {e}")
            return content

    def _estimate_tokens_used(self, content: str) -> int:
        """
        Estime le nombre de tokens utilis√©s.
        Approximation: 1 token ‚âà 4 caract√®res

        Args:
            content: Contenu √† estimer

        Returns:
            Nombre approximatif de tokens
        """
        if not content:
            return 0
        return max(1, len(content) // 4)

    async def cleanup(self):
        """Nettoie les ressources du service."""
        # Annuler les t√¢ches actives
        for task_id, task in self._active_tasks.items():
            if not task.done():
                task.cancel()
                self.logger.info(f"T√¢che {task_id} annul√©e")

        self._active_tasks.clear()

        # Nettoyer le navigateur
        await browser_manager.cleanup()


# Instance globale du service
scraping_service = ScrapingService()


async def scrape_url(task_data: ScrapingTaskCreate, **kwargs) -> dict[str, Any]:
    """
    Fonction utilitaire pour scraper une URL.

    Args:
        task_data: Donn√©es de la t√¢che
        **kwargs: Arguments additionnels

    Returns:
        R√©sultat du scraping
    """
    return await scraping_service.scrape_url(task_data, **kwargs)
