# Architecture Scrapinium - Guide D√©veloppeur

## üèóÔ∏è Vue d'Ensemble Architecture

Scrapinium suit une architecture modulaire enterprise-grade optimis√©e pour la performance, la scalabilit√© et la maintenabilit√©.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SCRAPINIUM ARCHITECTURE                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üåê Frontend (Web Interface)                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Templates Jinja2 + JavaScript vanilla                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Dashboard temps r√©el + Charts.js                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Interface scraping responsive                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üîå API Layer (FastAPI) - ARCHITECTURE MODULAIRE v0.4.0  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ routers/core.py (/, /health, /api)                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ routers/scraping.py (t√¢ches de scraping)            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ routers/statistics.py (monitoring syst√®me)          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ routers/cache.py (administration cache)             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ routers/maintenance.py (maintenance syst√®me)        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ routers/performance.py (optimisation perf)          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Middleware s√©curit√© (Rate limiting, Headers)        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Validation inputs + Serialization                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Background tasks + Progress tracking                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üß† Business Logic - COUCHE SERVICES v0.4.0              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ services/scraping_service.py (ScrapingTaskService)  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ LLM Integration (Ollama/OpenAI)                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ ML Pipeline (Intelligence Layer)                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Content Processing Pipeline                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Task Management (TaskManager thread-safe)          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Exception Hierarchy (gestion erreurs centralis√©e)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üé≠ Browser Pool Management                               ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Playwright Pool (3-5 instances)                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Queue management + Load balancing                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Resource monitoring + Auto-scaling                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Error handling + Recovery                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üíæ Cache Layer (Multi-niveau)                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Memory Cache (LRU + TTL)                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Redis Cache (Distributed)                          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Smart Cache Strategies                             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Cache invalidation + Warming                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üóÑÔ∏è Data Layer                                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ SQLAlchemy ORM + AsyncIO                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ PostgreSQL (Production)                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ SQLite (Development)                               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Migration management                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üõ°Ô∏è Security Layer                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Rate Limiting + DoS Protection                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Input Validation + Sanitization                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Security Headers + CORS                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Audit + Compliance                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìä Observability                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Memory Monitoring + Metrics                       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Performance tracking                              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Structured Logging                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Health checks                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üèõÔ∏è Modules Principaux

### 1. API Layer (`src/scrapinium/api/`) - ARCHITECTURE MODULAIRE v0.4.0

**Architecture FastAPI modulaire avec routers sp√©cialis√©s et couche services**

```python
api/
‚îú‚îÄ‚îÄ app.py              # Application principale (149 lignes - orchestrateur)
‚îú‚îÄ‚îÄ routers/            # Routers modulaires par domaine (v0.4.0)
‚îÇ   ‚îú‚îÄ‚îÄ core.py        # Endpoints racine (/, /health, /api)
‚îÇ   ‚îú‚îÄ‚îÄ scraping.py    # Gestion t√¢ches de scraping
‚îÇ   ‚îú‚îÄ‚îÄ statistics.py  # Monitoring et m√©triques syst√®me
‚îÇ   ‚îú‚îÄ‚îÄ cache.py       # Administration cache multi-niveau
‚îÇ   ‚îú‚îÄ‚îÄ maintenance.py # Op√©rations de maintenance syst√®me
‚îÇ   ‚îî‚îÄ‚îÄ performance.py # Surveillance et optimisation
‚îú‚îÄ‚îÄ services/           # Couche services business logic (v0.4.0)
‚îÇ   ‚îî‚îÄ‚îÄ scraping_service.py # ScrapingTaskService avec patterns
‚îú‚îÄ‚îÄ task_manager.py     # Gestionnaire thread-safe des t√¢ches
‚îú‚îÄ‚îÄ ml_manager.py       # Gestionnaire ML avec singleton
‚îú‚îÄ‚îÄ exception_handler.py # Gestion d'exceptions centralis√©e
‚îî‚îÄ‚îÄ validators.py       # Validation inputs s√©curis√©e
```

**Patterns architecturaux v0.4.0:**
- ‚úÖ **Router Pattern** : S√©paration par domaine fonctionnel
- ‚úÖ **Service Layer** : Logique m√©tier extraite et centralis√©e  
- ‚úÖ **Singleton Pattern** : Gestionnaires avec instances uniques
- ‚úÖ **Dependency Injection** avec FastAPI
- ‚úÖ **Exception Hierarchy** : Gestion d'erreurs typ√©e et centralis√©e
- ‚úÖ **Thread-Safe Managers** : TaskManager et MLManager avec RLock
- ‚úÖ **Async/Await** pour performance optimale
- ‚úÖ **Background Tasks** pour t√¢ches longues
- ‚úÖ **Middleware Pipeline** pour s√©curit√©
- ‚úÖ **Request/Response Models** avec Pydantic

**Am√©liorations v0.4.0:**
- **R√©duction complexit√©** : app.py 1071 ‚Üí 149 lignes (-86%)
- **Maintenabilit√©** : Code modulaire avec responsabilit√©s s√©par√©es
- **Testabilit√©** : Services isol√©s et facilement mockables
- **√âvolutivit√©** : Ajout de fonctionnalit√©s simplifi√©
- **Code Quality** : Respect des principes SOLID et Clean Code

### 2. Scraping Engine (`src/scrapinium/scraping/`)

**Moteur de scraping haute performance avec pool de navigateurs**

```python
scraping/
‚îú‚îÄ‚îÄ service.py          # Orchestrateur principal
‚îú‚îÄ‚îÄ browser.py          # Pool de navigateurs Playwright
‚îú‚îÄ‚îÄ extractor.py        # Extraction de contenu
‚îú‚îÄ‚îÄ processor.py        # Traitement de donn√©es
‚îú‚îÄ‚îÄ strategies/         # Strat√©gies d'extraction
‚îÇ   ‚îú‚îÄ‚îÄ html.py        # Extraction HTML/CSS
‚îÇ   ‚îú‚îÄ‚îÄ javascript.py  # Rendu JavaScript
‚îÇ   ‚îî‚îÄ‚îÄ adaptive.py    # Strat√©gie adaptative
‚îî‚îÄ‚îÄ utils.py           # Utilitaires scraping
```

**Architecture Browser Pool:**

```python
class BrowserPool:
    """Pool de navigateurs optimis√© pour performance"""
    
    def __init__(self, max_browsers: int = 5):
        self.browsers: Queue[Browser] = Queue()
        self.active_browsers: Dict[str, BrowserInstance] = {}
        self.stats = BrowserPoolStats()
        
    async def acquire_browser(self) -> BrowserContext:
        """Acquisition thread-safe d'un navigateur"""
        
    async def release_browser(self, browser: BrowserContext):
        """Lib√©ration et nettoyage automatique"""
        
    def get_pool_stats(self) -> BrowserPoolStats:
        """M√©triques temps r√©el du pool"""
```

### 3. LLM Integration (`src/scrapinium/llm/`)

**Int√©gration multi-providers avec fallback intelligents**

```python
llm/
‚îú‚îÄ‚îÄ providers/          # Providers LLM
‚îÇ   ‚îú‚îÄ‚îÄ ollama.py      # Ollama local
‚îÇ   ‚îú‚îÄ‚îÄ openai.py      # OpenAI API
‚îÇ   ‚îú‚îÄ‚îÄ anthropic.py   # Claude API
‚îÇ   ‚îî‚îÄ‚îÄ google.py      # Gemini API
‚îú‚îÄ‚îÄ manager.py          # Gestionnaire multi-providers
‚îú‚îÄ‚îÄ prompts/           # Templates de prompts
‚îú‚îÄ‚îÄ cache.py           # Cache sp√©cialis√© LLM
‚îî‚îÄ‚îÄ streaming.py       # Streaming responses
```

**Pattern Provider:**

```python
class LLMProvider(Protocol):
    """Interface commune pour tous les providers LLM"""
    
    async def health_check(self) -> bool:
        """V√©rification de sant√© du provider"""
        
    async def process_content(
        self, 
        content: str, 
        instructions: str = "",
        model: str = "default"
    ) -> LLMResponse:
        """Traitement de contenu avec LLM"""
        
    async def stream_process(
        self, 
        content: str, 
        instructions: str = ""
    ) -> AsyncIterator[str]:
        """Traitement en streaming"""
```

### 4. Cache System (`src/scrapinium/cache/`)

**Syst√®me de cache multi-niveau avec strat√©gies intelligentes**

```python
cache/
‚îú‚îÄ‚îÄ manager.py          # Gestionnaire principal
‚îú‚îÄ‚îÄ backends/          # Impl√©mentations cache
‚îÇ   ‚îú‚îÄ‚îÄ memory.py      # Cache m√©moire LRU
‚îÇ   ‚îú‚îÄ‚îÄ redis.py       # Cache Redis distribu√©
‚îÇ   ‚îî‚îÄ‚îÄ hybrid.py      # Cache hybride intelligent
‚îú‚îÄ‚îÄ strategies/        # Strat√©gies de cache
‚îÇ   ‚îú‚îÄ‚îÄ lru.py        # Least Recently Used
‚îÇ   ‚îú‚îÄ‚îÄ ttl.py        # Time To Live
‚îÇ   ‚îú‚îÄ‚îÄ smart.py      # Strat√©gie intelligente
‚îÇ   ‚îî‚îÄ‚îÄ adaptive.py   # Cache adaptatif
‚îú‚îÄ‚îÄ serializers.py     # S√©rialiseurs optimis√©s
‚îî‚îÄ‚îÄ invalidation.py    # Invalidation intelligente
```

**Cache Multi-Niveau:**

```python
class CacheManager:
    """Gestionnaire de cache multi-niveau"""
    
    def __init__(self):
        self.l1_cache = MemoryCache(max_size=1000)  # L1: M√©moire
        self.l2_cache = RedisCache(url=redis_url)   # L2: Redis
        self.strategy = SmartCacheStrategy()
        
    async def get(self, key: str) -> Optional[Any]:
        """R√©cup√©ration avec fallback L1 ‚Üí L2"""
        
    async def set(self, key: str, value: Any, ttl: int = 3600):
        """Stockage intelligent multi-niveau"""
        
    async def invalidate(self, pattern: str):
        """Invalidation par pattern"""
```

### 5. Security Layer (`src/scrapinium/security/`)

**S√©curit√© enterprise avec conformit√© standards**

```python
security/
‚îú‚îÄ‚îÄ rate_limiter.py     # Rate limiting avanc√©
‚îú‚îÄ‚îÄ input_validator.py  # Validation inputs stricte
‚îú‚îÄ‚îÄ headers.py          # Headers s√©curit√©
‚îú‚îÄ‚îÄ config_security.py # Configuration s√©curis√©e
‚îú‚îÄ‚îÄ auth/              # Authentification (future)
‚îú‚îÄ‚îÄ monitoring/        # Monitoring s√©curit√©
‚îî‚îÄ‚îÄ compliance/        # Conformit√© standards
```

**Pattern Security Middleware:**

```python
async def security_pipeline(request: Request, call_next):
    """Pipeline de s√©curit√© multi-√©tapes"""
    
    # 1. Rate limiting
    await rate_limiter.check_limits(request)
    
    # 2. Input validation
    await input_validator.validate_request(request)
    
    # 3. Traitement
    response = await call_next(request)
    
    # 4. Security headers
    response = security_headers.apply_headers(response)
    
    return response
```

### 6. ML Pipeline (`src/scrapinium/ml/`)

**Intelligence artificielle int√©gr√©e pour un scraping adaptatif**

```python
ml/
‚îú‚îÄ‚îÄ ml_pipeline.py      # Pipeline ML principal
‚îú‚îÄ‚îÄ content_classifier.py # Classification contenu 
‚îú‚îÄ‚îÄ antibot_detector.py   # D√©tection anti-bot
‚îú‚îÄ‚îÄ content_analyzer.py   # Analyse s√©mantique
‚îú‚îÄ‚îÄ models/              # Mod√®les ML (future)
‚îî‚îÄ‚îÄ training/           # Training pipeline (future)
```

**Pipeline ML Pattern:**

```python
class MLPipeline:
    """Pipeline ML complet avec cache et parall√©lisation"""
    
    async def analyze_page(self, html: str, url: str) -> MLAnalysisResult:
        """Analyse intelligente d'une page web"""
        
        # Cache check
        cache_key = self._generate_cache_key(html, url)
        cached = self._get_from_cache(cache_key)
        if cached:
            return cached
        
        # Analyses parall√®les pour performance
        classification_task = asyncio.create_task(
            self.content_classifier.classify_page(html, url)
        )
        
        bot_detection_task = asyncio.create_task(
            self.antibot_detector.analyze_page(html, headers, url)
        )
        
        classification, bot_detection = await asyncio.gather(
            classification_task, bot_detection_task
        )
        
        # Analyse s√©mantique
        content_features = await self.content_analyzer.analyze_content(
            html, classification.features['text_content'], url
        )
        
        # Construction du r√©sultat
        result = MLAnalysisResult(
            classification=classification,
            bot_detection=bot_detection, 
            content_features=content_features,
            # ... autres champs
        )
        
        # Cache storage
        self._store_in_cache(cache_key, result)
        return result
```

**Analyseurs Sp√©cialis√©s:**

```python
# ContentClassifier - Classification de contenu
class ContentClassifier:
    def classify_page(self, html: str, url: str) -> ClassificationResult:
        """Classifie le type et la qualit√© du contenu"""
        # Types: article, ecommerce, blog, forum, news, documentation
        # Qualit√©: high, medium, low, spam
        # Langue: fr, en, es, unknown

# AntibotDetector - D√©tection d√©fis anti-bot  
class AntibotDetector:
    def analyze_page(self, html: str, headers: Dict) -> DetectionResult:
        """D√©tecte les d√©fis anti-bot et g√©n√®re strat√©gies d'√©vasion"""
        # D√©fis: cloudflare, recaptcha, rate_limiting, js_challenge
        # Strat√©gies: stealth_mode, rotation, delay_randomization

# ContentAnalyzer - Analyse s√©mantique
class ContentAnalyzer:
    def analyze_content(self, html: str, text: str) -> ContentFeatures:
        """Analyse s√©mantique compl√®te du contenu"""
        # M√©triques: mots, lisibilit√©, sentiment, topics, keywords
        # Structure: titres, listes, tableaux, m√©dias
```

## üîÑ Flux de Donn√©es

### Workflow Scraping Complet

```mermaid
graph TB
    A[üåê Requ√™te HTTP] --> B[üõ°Ô∏è Security Middleware]
    B --> C[üìù Validation Input]
    C --> D[üíæ Cache Check]
    D --> E{Cache Hit?}
    E -->|Oui| F[üì§ R√©ponse Imm√©diate]
    E -->|Non| G[üé≠ Browser Pool]
    G --> H[üîç Content Extraction]
    H --> I[üß† ML Analysis Pipeline]
    I --> J[üß† LLM Processing]
    J --> K[üìä Content Processing]
    K --> L[üíæ Cache Storage]
    L --> M[üì§ R√©ponse Finale]
    
    I --> I1[üîç Content Classification]
    I --> I2[üõ°Ô∏è Anti-bot Detection] 
    I --> I3[üìä Content Analysis]
    I1 --> J
    I2 --> J
    I3 --> J
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style G fill:#fff3e0
    style I fill:#e8f5e8
    style I1 fill:#e3f2fd
    style I2 fill:#f1f8e9
    style I3 fill:#fff8e1
    style J fill:#e8f5e8
    style M fill:#fce4ec
```

### Cycle de Vie d'une T√¢che

```python
class ScrapingTask:
    """Cycle de vie complet d'une t√¢che de scraping"""
    
    async def execute(self):
        try:
            # 1. Initialisation
            self.status = TaskStatus.INITIALIZING
            await self.validate_input()
            
            # 2. Cache check
            cached_result = await self.check_cache()
            if cached_result:
                return cached_result
            
            # 3. Browser acquisition
            self.status = TaskStatus.ACQUIRING_BROWSER
            browser = await self.browser_pool.acquire()
            
            # 4. Content extraction
            self.status = TaskStatus.EXTRACTING
            raw_content = await self.extract_content(browser)
            
            # 5. LLM processing
            if self.use_llm:
                self.status = TaskStatus.PROCESSING_LLM
                structured_content = await self.llm_process(raw_content)
            else:
                structured_content = raw_content
            
            # 6. Post-processing
            self.status = TaskStatus.POST_PROCESSING
            final_content = await self.post_process(structured_content)
            
            # 7. Cache storage
            await self.store_in_cache(final_content)
            
            # 8. Completion
            self.status = TaskStatus.COMPLETED
            return final_content
            
        except Exception as e:
            self.status = TaskStatus.FAILED
            await self.handle_error(e)
            raise
        finally:
            await self.cleanup()
```

## üóÑÔ∏è Mod√®les de Donn√©es

### Core Models

```python
# src/scrapinium/models/schemas.py

class ScrapingTaskCreate(BaseModel):
    """Mod√®le de cr√©ation de t√¢che"""
    url: HttpUrl
    output_format: OutputFormat = OutputFormat.MARKDOWN
    use_llm: bool = False
    custom_instructions: Optional[str] = None
    use_cache: bool = True
    priority: TaskPriority = TaskPriority.NORMAL
    
    @validator("url")
    def validate_url_security(cls, v):
        """Validation s√©curis√©e de l'URL"""
        # Validation anti-SSRF, domaines bloqu√©s, etc.
        return security_validator.validate_url(v)

class ScrapingResult(BaseModel):
    """R√©sultat de scraping avec m√©tadonn√©es"""
    id: str
    url: str
    status: TaskStatus
    structured_content: Optional[str]
    metadata: TaskMetadata
    created_at: datetime
    completed_at: Optional[datetime]
    
class TaskMetadata(BaseModel):
    """M√©tadonn√©es riches de la t√¢che"""
    execution_time_ms: int
    tokens_used: Optional[int]
    content_length: int
    word_count: int
    reading_time_minutes: float
    cache_hit: bool
    browser_used: str
    llm_provider: Optional[str]
    security_score: float
```

### Database Schema

```sql
-- Structure optimis√©e pour performance
CREATE TABLE scraping_tasks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT NOT NULL,
    status task_status NOT NULL DEFAULT 'pending',
    structured_content TEXT,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    completed_at TIMESTAMP WITH TIME ZONE,
    user_id UUID,
    
    -- Index pour performance
    INDEX idx_tasks_status ON scraping_tasks(status),
    INDEX idx_tasks_created ON scraping_tasks(created_at DESC),
    INDEX idx_tasks_url_hash ON scraping_tasks(md5(url))
);

-- Table d'audit s√©curit√©
CREATE TABLE security_events (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    event_type TEXT NOT NULL,
    severity security_severity NOT NULL,
    client_id TEXT,
    ip_address INET,
    user_agent TEXT,
    details JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

## ‚ö° Patterns de Performance

### 1. Async/Await Optimis√©

```python
class AsyncOptimizedService:
    """Service optimis√© pour performance async"""
    
    async def process_multiple_urls(
        self, 
        urls: List[str],
        concurrency_limit: int = 10
    ) -> List[ScrapingResult]:
        """Traitement concurrent optimis√©"""
        
        semaphore = asyncio.Semaphore(concurrency_limit)
        
        async def process_single_url(url: str) -> ScrapingResult:
            async with semaphore:
                return await self.scrape_url(url)
        
        # Execution en batch pour √©viter l'overhead
        tasks = [process_single_url(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [r for r in results if isinstance(r, ScrapingResult)]
```

### 2. Memory Management Avanc√©

```python
class MemoryOptimizedProcessor:
    """Traitement optimis√© m√©moire avec streaming"""
    
    async def process_large_content(
        self, 
        content: str,
        chunk_size: int = 1024 * 1024  # 1MB chunks
    ) -> AsyncIterator[str]:
        """Traitement par chunks pour gros contenus"""
        
        for i in range(0, len(content), chunk_size):
            chunk = content[i:i + chunk_size]
            processed_chunk = await self.process_chunk(chunk)
            
            # Yield imm√©diatement pour lib√©rer la m√©moire
            yield processed_chunk
            
            # Force garbage collection si n√©cessaire
            if i % (chunk_size * 10) == 0:
                await asyncio.sleep(0)  # Allow event loop
                gc.collect()
```

### 3. Cache Intelligent

```python
class SmartCacheStrategy:
    """Strat√©gie de cache intelligente bas√©e sur l'usage"""
    
    def calculate_cache_priority(
        self, 
        url: str, 
        content_size: int,
        access_frequency: int
    ) -> CachePriority:
        """Calcul intelligent de priorit√© de cache"""
        
        # Facteurs de d√©cision
        size_factor = 1.0 / (1 + content_size / 1024**2)  # P√©nalit√© taille
        freq_factor = min(access_frequency / 10, 2.0)      # Bonus fr√©quence
        time_factor = self.get_temporal_factor(url)        # Tendance temporelle
        
        priority_score = size_factor * freq_factor * time_factor
        
        if priority_score > 1.5:
            return CachePriority.HIGH
        elif priority_score > 0.8:
            return CachePriority.MEDIUM
        else:
            return CachePriority.LOW
```

## üîß Configuration Avanc√©e

### Environment Configuration

```python
# src/scrapinium/config/settings.py

class Settings(BaseSettings):
    """Configuration centralis√©e avec validation"""
    
    # === CORE ===
    app_name: str = "Scrapinium"
    app_version: str = "2.0.0"
    debug: bool = False
    environment: Environment = Environment.PRODUCTION
    
    # === DATABASE ===
    database_url: PostgresDsn
    database_pool_size: int = 20
    database_max_overflow: int = 30
    database_echo: bool = False
    
    # === CACHE ===
    redis_url: Optional[RedisDsn] = None
    cache_default_ttl: int = 3600
    cache_max_memory_mb: int = 512
    
    # === BROWSER POOL ===
    browser_pool_size: int = 5
    browser_timeout_seconds: int = 30
    browser_max_age_minutes: int = 60
    
    # === LLM ===
    ollama_host: str = "http://localhost:11434"
    openai_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
    
    # === SECURITY ===
    security_level: SecurityLevel = SecurityLevel.PRODUCTION
    rate_limit_per_minute: int = 60
    rate_limit_per_hour: int = 1000
    rate_limit_burst: int = 10
    
    # === PERFORMANCE ===
    max_concurrent_tasks: int = 100
    task_timeout_seconds: int = 300
    memory_limit_mb: int = 2048
    
    class Config:
        env_file = ".env"
        case_sensitive = False
        
    @validator("database_url")
    def validate_database_url(cls, v):
        """Validation s√©curis√©e de l'URL de base de donn√©es"""
        if not v.startswith(("postgresql://", "sqlite://")):
            raise ValueError("URL de base de donn√©es non support√©e")
        return v
```

### Configuration par Environnement

```yaml
# config/environments/production.yaml
database:
  pool_size: 50
  max_overflow: 100
  echo: false
  
browser_pool:
  size: 10
  timeout: 45
  max_age: 30
  
cache:
  ttl: 7200
  max_memory: 1024
  
security:
  level: "enterprise"
  rate_limit_per_minute: 30
  strict_validation: true
  
monitoring:
  metrics_enabled: true
  health_check_interval: 30
  log_level: "INFO"
```

## üîÑ Extensibilit√©

### Plugin Architecture

```python
# src/scrapinium/plugins/

class ScrapiniumPlugin(ABC):
    """Interface pour plugins Scrapinium"""
    
    @abstractmethod
    def initialize(self, app: FastAPI) -> None:
        """Initialisation du plugin"""
        
    @abstractmethod
    def register_routes(self, router: APIRouter) -> None:
        """Enregistrement des routes"""
        
    @abstractmethod
    def get_info(self) -> PluginInfo:
        """Informations du plugin"""

class CustomExtractorPlugin(ScrapiniumPlugin):
    """Plugin d'extraction personnalis√©"""
    
    def initialize(self, app: FastAPI) -> None:
        # Ajouter middleware, services, etc.
        pass
        
    def register_routes(self, router: APIRouter) -> None:
        @router.post("/custom/extract")
        async def custom_extract(data: CustomExtractRequest):
            return await self.extract(data)
```

### Provider Pattern pour LLM

```python
class CustomLLMProvider(LLMProvider):
    """Provider LLM personnalis√©"""
    
    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url
        self.client = CustomLLMClient(api_key, base_url)
    
    async def health_check(self) -> bool:
        try:
            await self.client.ping()
            return True
        except Exception:
            return False
    
    async def process_content(
        self, 
        content: str, 
        instructions: str = "",
        model: str = "default"
    ) -> LLMResponse:
        response = await self.client.complete(
            prompt=f"{instructions}\n\nContent: {content}",
            model=model
        )
        
        return LLMResponse(
            content=response.text,
            tokens_used=response.usage.total_tokens,
            model=model,
            provider="custom"
        )

# Enregistrement du provider
llm_manager.register_provider("custom", CustomLLMProvider)
```

## üìä Monitoring et Observabilit√©

### M√©triques Custom

```python
class MetricsCollector:
    """Collecteur de m√©triques personnalis√©"""
    
    def __init__(self):
        self.counters = defaultdict(int)
        self.histograms = defaultdict(list)
        self.gauges = defaultdict(float)
    
    def increment(self, metric: str, value: int = 1, labels: Dict = None):
        """Compteur incr√©mental"""
        key = self._make_key(metric, labels)
        self.counters[key] += value
    
    def record_duration(self, metric: str, duration_ms: float, labels: Dict = None):
        """Enregistrement de dur√©e"""
        key = self._make_key(metric, labels)
        self.histograms[key].append(duration_ms)
    
    def set_gauge(self, metric: str, value: float, labels: Dict = None):
        """M√©trique gauge"""
        key = self._make_key(metric, labels)
        self.gauges[key] = value
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """R√©sum√© des m√©triques"""
        return {
            "counters": dict(self.counters),
            "histograms": {
                k: {
                    "count": len(v),
                    "avg": sum(v) / len(v) if v else 0,
                    "p95": self._percentile(v, 95) if v else 0,
                    "p99": self._percentile(v, 99) if v else 0
                }
                for k, v in self.histograms.items()
            },
            "gauges": dict(self.gauges)
        }

# D√©corateur pour mesurer automatiquement
def measure_performance(metric_name: str):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                metrics_collector.increment(f"{metric_name}.success")
                return result
            except Exception as e:
                metrics_collector.increment(f"{metric_name}.error")
                raise
            finally:
                duration = (time.time() - start_time) * 1000
                metrics_collector.record_duration(metric_name, duration)
        return wrapper
    return decorator
```

---

**Version**: 2.0.0  
**Derni√®re mise √† jour**: 2024-12-21  
**Auteur**: √âquipe Scrapinium  
**Licence**: Open Source