# Guide de Tests - Scrapinium

## ğŸ§ª Framework de Tests Enterprise

Scrapinium dispose d'un framework de tests complet conÃ§u pour assurer la qualitÃ© et la robustesse du code en production.

## ğŸ“‹ Structure des Tests

```
tests/
â”œâ”€â”€ conftest.py                    # Configuration pytest & fixtures
â”œâ”€â”€ pytest.ini                    # Configuration markers & options
â”œâ”€â”€ test_api_integration.py       # Tests API REST complÃ¨tes
â”œâ”€â”€ test_browser_pool.py          # Tests du pool de navigateurs
â”œâ”€â”€ test_cache_system.py          # Tests systÃ¨me de cache multi-niveau
â”œâ”€â”€ test_e2e_scenarios.py         # Tests de scÃ©narios end-to-end
â”œâ”€â”€ test_memory_management.py     # Tests gestion mÃ©moire
â””â”€â”€ test_security.py              # Tests de sÃ©curitÃ©
```

## ğŸ·ï¸ Markers de Tests

| Marker | Description | Usage |
|--------|-------------|-------|
| `unit` | Tests unitaires rapides | Tests isolÃ©s, mocks |
| `integration` | Tests d'intÃ©gration | Tests API, composants |
| `performance` | Tests de performance | Benchmarks, temps de rÃ©ponse |
| `slow` | Tests lents | Tests nÃ©cessitant des ressources |
| `api` | Tests API REST | Endpoints, validation |
| `scraping` | Tests scraping web | Pool navigateurs, extraction |
| `cache` | Tests cache | Redis, mÃ©moire, stratÃ©gies |
| `memory` | Tests mÃ©moire | Surveillance, optimisation |
| `security` | Tests sÃ©curitÃ© | XSS, injection SQL, validation |

## ğŸš€ Lancement des Tests

### Script de Test IntÃ©grÃ©

```bash
# Tests rapides (recommandÃ© pour dÃ©veloppement)
python scripts/run_tests.py --level fast

# Tests unitaires uniquement
python scripts/run_tests.py --level unit

# Tests d'intÃ©gration
python scripts/run_tests.py --level integration

# Tous les tests
python scripts/run_tests.py --level all

# Tests avec couverture
python scripts/run_tests.py --level fast --coverage

# Tests en parallÃ¨le
python scripts/run_tests.py --level fast --parallel

# Tests par marker
python scripts/run_tests.py --markers "api and not slow"
```

### Commandes Pytest Directes

```bash
# Tests par marker
pytest -m "unit and not slow"
pytest -m "integration"
pytest -m "performance"
pytest -m "security"

# Tests par fichier
pytest tests/test_api_integration.py -v
pytest tests/test_security.py -v

# Tests avec couverture
pytest --cov=src/scrapinium --cov-report=html

# Tests en parallÃ¨le
pytest -n auto

# Tests avec output dÃ©taillÃ©
pytest -v --tb=long
```

## ğŸ“Š CatÃ©gories de Tests

### 1. Tests Unitaires

**Fichiers**: `test_browser_pool.py`, `test_cache_system.py`, `test_memory_management.py`

- Tests isolÃ©s avec mocks
- Validation de logique mÃ©tier
- Calculs et algorithmes
- Configuration et validation

**Exemple**:
```python
@pytest.mark.unit
def test_cache_hit_rate_calculation(self):
    requests = [{"hit": True}, {"hit": False}, {"hit": True}]
    hit_rate = calculate_hit_rate(requests)
    assert hit_rate == 66.67
```

### 2. Tests d'IntÃ©gration

**Fichiers**: `test_api_integration.py`, `test_e2e_scenarios.py`

- Tests API end-to-end
- IntÃ©gration des composants
- Workflows complets
- Validation des endpoints

**Exemple**:
```python
@pytest.mark.integration
def test_complete_scraping_workflow(self, client):
    # 1. CrÃ©er tÃ¢che
    response = client.post("/scrape", json={"url": "https://example.com"})
    task_id = response.json()["data"]["task_id"]
    
    # 2. VÃ©rifier statut
    status = client.get(f"/scrape/{task_id}")
    assert status.status_code == 200
```

### 3. Tests de SÃ©curitÃ©

**Fichier**: `test_security.py`

- Protection XSS
- Injection SQL
- Validation des entrÃ©es
- Headers de sÃ©curitÃ©
- Sanitisation des donnÃ©es

**Exemple**:
```python
@pytest.mark.security
def test_sql_injection_protection(self, client):
    malicious_payload = "'; DROP TABLE users; --"
    response = client.get(f"/scrape/{malicious_payload}")
    assert response.status_code == 404  # Pas 500
```

### 4. Tests de Performance

**IntÃ©grÃ©s dans chaque fichier**

- Temps de rÃ©ponse API
- Utilisation mÃ©moire
- EfficacitÃ© du cache
- Concurrence

**Exemple**:
```python
@pytest.mark.performance
def test_api_response_time(self, client, performance_thresholds):
    start_time = time.time()
    response = client.get("/health")
    response_time = (time.time() - start_time) * 1000
    
    assert response_time < performance_thresholds["api_response_time_ms"]
```

## ğŸ”§ Configuration des Tests

### Fixtures Principales

```python
# Client de test FastAPI
@pytest.fixture
def client(app) -> TestClient:
    return TestClient(app)

# Seuils de performance
@pytest.fixture
def performance_thresholds():
    return {
        "api_response_time_ms": 100,
        "memory_usage_max_mb": 512,
        "cache_hit_rate_min": 80.0
    }

# URLs de test
@pytest.fixture
def sample_urls():
    return [
        "https://httpbin.org/html",
        "https://example.com"
    ]
```

### ParamÃ¨tres de Test

```ini
# pytest.ini
[tool:pytest]
testpaths = tests
markers =
    unit: Tests unitaires rapides
    integration: Tests d'intÃ©gration
    performance: Tests de performance
    security: Tests de sÃ©curitÃ©
    
addopts = -v --tb=short --strict-markers
timeout = 60
```

## ğŸ“ˆ Couverture de Code

### GÃ©nÃ©ration des Rapports

```bash
# Rapport HTML (recommandÃ©)
pytest --cov=src/scrapinium --cov-report=html
open htmlcov/index.html

# Rapport terminal
pytest --cov=src/scrapinium --cov-report=term

# Rapport complet
pytest --cov=src/scrapinium --cov-report=html --cov-report=term --cov-report=xml
```

### Objectifs de Couverture

| Composant | Objectif | PrioritÃ© |
|-----------|----------|----------|
| API Core | 95%+ | Critique |
| Cache System | 90%+ | Haute |
| Browser Pool | 85%+ | Haute |
| Memory Management | 80%+ | Moyenne |
| Utilities | 75%+ | Moyenne |

## ğŸš¨ Tests de RÃ©gression

### Avant Chaque Release

```bash
# Suite complÃ¨te de tests
python scripts/run_tests.py --level all --coverage

# Tests de sÃ©curitÃ©
pytest -m security -v

# Tests de performance
pytest -m performance -v

# Tests end-to-end
pytest -m "integration and slow" -v
```

### CI/CD Pipeline

```yaml
# .github/workflows/tests.yml (exemple)
- name: Run Fast Tests
  run: python scripts/run_tests.py --level fast

- name: Run Security Tests  
  run: pytest -m security

- name: Run Performance Tests
  run: pytest -m performance
```

## ğŸ” Debugging des Tests

### Modes de Debug

```bash
# Mode verbeux complet
pytest -vvv --tb=long

# ArrÃªt au premier Ã©chec
pytest -x

# Debug avec pdb
pytest --pdb

# Logs dÃ©taillÃ©s
pytest --log-cli-level=DEBUG
```

### Analyse des Ã‰checs

```bash
# Re-exÃ©cuter les tests Ã©chouÃ©s
pytest --lf

# ExÃ©cuter les tests les plus lents
pytest --durations=10

# Profiling des tests
pytest --profile
```

## ğŸ“š Bonnes Pratiques

### Structure des Tests

1. **AAA Pattern**: Arrange, Act, Assert
2. **Noms descriptifs**: `test_api_returns_valid_response_for_valid_url`
3. **Tests isolÃ©s**: Pas de dÃ©pendances entre tests
4. **Mocks appropriÃ©s**: Pour les services externes

### Performance

1. **Tests rapides**: Unitaires < 100ms
2. **Tests parallÃ¨les**: Utiliser `-n auto` pour CI
3. **Fixtures scoped**: RÃ©utiliser les fixtures coÃ»teuses
4. **Skip intelligents**: Tests dÃ©pendants des services externes

### Maintenance

1. **Nettoyage**: Supprimer les tests obsolÃ¨tes
2. **Refactoring**: Maintenir la lisibilitÃ©
3. **Documentation**: Commenter les tests complexes
4. **Mise Ã  jour**: Adapter aux Ã©volutions du code

## ğŸ“Š MÃ©triques de QualitÃ©

### Objectifs de Performance

| MÃ©trique | Objectif | Seuil Critique |
|----------|----------|---------------|
| Tests unitaires | < 50ms | 100ms |
| Tests intÃ©gration | < 500ms | 1000ms |
| Couverture globale | > 85% | < 70% |
| Taux de succÃ¨s CI | > 98% | < 95% |

### Surveillance Continue

- **Temps d'exÃ©cution**: Monitoring des performances
- **Flaky tests**: DÃ©tection des tests instables  
- **Couverture**: Ã‰volution de la couverture
- **QualitÃ©**: Code smells et complexitÃ©

---

**Version**: 0.2.0  
**DerniÃ¨re mise Ã  jour**: 2024-12-21  
**Auteur**: Scrapinium Team