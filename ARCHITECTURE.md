# üèóÔ∏è Architecture de Scrapinium

> Architecture hexagonale moderne pour le scraping web intelligent avec LLMs

## üìã Vue d'ensemble

Scrapinium adopte une **architecture hexagonale** (ports et adaptateurs) qui s√©pare clairement la logique m√©tier des d√©tails d'impl√©mentation. Cette approche garantit :

- ‚úÖ **Testabilit√©** : Isolation des composants pour tests unitaires
- ‚úÖ **Maintenabilit√©** : Code modulaire et d√©coupl√©
- ‚úÖ **√âvolutivit√©** : Facilit√© d'ajout de nouvelles fonctionnalit√©s
- ‚úÖ **Flexibilit√©** : Changement d'impl√©mentation sans impact sur le c≈ìur

## üîÑ Diagramme d'architecture

```mermaid
graph TB
    subgraph "Interfaces Externes"
        UI[Interface Reflex]
        API[API FastAPI]
        CLI[CLI Commands]
    end
    
    subgraph "Couche Application"
        Routes[Routes & Controllers]
        Services[Services Applicatifs]
    end
    
    subgraph "C≈ìur M√©tier"
        ScrapingCore[Service de Scraping]
        LLMCore[Service LLM]
        TaskCore[Gestion des T√¢ches]
    end
    
    subgraph "Adaptateurs Sortants"
        Browser[Gestionnaire Playwright]
        Database[Gestionnaire DB]
        LLMClient[Client Ollama]
        Cache[Cache Redis]
    end
    
    subgraph "Infrastructure"
        Playwright[Playwright Browser]
        SQLite[(Base de Donn√©es)]
        Ollama[Ollama LLM]
        Redis[(Cache Redis)]
    end
    
    UI --> Routes
    API --> Routes
    CLI --> Services
    Routes --> Services
    Services --> ScrapingCore
    Services --> LLMCore
    Services --> TaskCore
    
    ScrapingCore --> Browser
    LLMCore --> LLMClient
    TaskCore --> Database
    TaskCore --> Cache
    
    Browser --> Playwright
    Database --> SQLite
    LLMClient --> Ollama
    Cache --> Redis
```

## üìÅ Structure d√©taill√©e

```
src/scrapinium/
‚îú‚îÄ‚îÄ üåê api/                    # Interface API (Port d'entr√©e)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ app.py                 # Application FastAPI & routes
‚îÇ
‚îú‚îÄ‚îÄ üé® ui/                     # Interface utilisateur (Port d'entr√©e)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ app.py                 # Application Reflex principale
‚îÇ   ‚îú‚îÄ‚îÄ components/            # Composants r√©utilisables
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forms.py          # Formulaires de saisie
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tables.py         # Tableaux de donn√©es
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cards.py          # Cartes d'affichage
‚îÇ   ‚îî‚îÄ‚îÄ styles.py             # Syst√®me de th√®me sombre
‚îÇ
‚îú‚îÄ‚îÄ üï∏Ô∏è scraping/              # C≈ìur m√©tier - Service de scraping
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ browser.py            # Gestionnaire Playwright (Adaptateur)
‚îÇ   ‚îú‚îÄ‚îÄ extractor.py          # Extracteur de contenu avec Readability
‚îÇ   ‚îî‚îÄ‚îÄ service.py            # Service principal de scraping
‚îÇ
‚îú‚îÄ‚îÄ üß† llm/                   # C≈ìur m√©tier - Service LLM
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ ollama.py             # Client Ollama (Adaptateur)
‚îÇ
‚îú‚îÄ‚îÄ üóÉÔ∏è models/                # Mod√®les de domaine
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ database.py           # Entit√©s SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ enums.py              # √ânum√©rations m√©tier
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py            # DTOs Pydantic
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/                # Configuration syst√®me
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ database.py           # Configuration base de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ settings.py           # Configuration globale
‚îÇ
‚îî‚îÄ‚îÄ üõ†Ô∏è utils/                 # Utilitaires transversaux
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ helpers.py            # Fonctions utilitaires
    ‚îî‚îÄ‚îÄ validators.py         # Validateurs de donn√©es
```

## üèõÔ∏è Principes architecturaux

### 1. S√©paration des responsabilit√©s

**Couche Interface (Ports d'entr√©e)**
- `api/` : Exposition HTTP REST avec FastAPI
- `ui/` : Interface utilisateur avec Reflex
- Responsabilit√© : Traduction des demandes externes vers le domaine

**Couche Application**
- Routes et contr√¥leurs dans `api/app.py`
- Orchestration des services m√©tier
- Responsabilit√© : Coordination et workflow

**Couche Domaine (C≈ìur m√©tier)**
- `scraping/service.py` : Logique de scraping
- `llm/` : Logique d'intelligence artificielle
- Responsabilit√© : R√®gles m√©tier et invariants

**Couche Infrastructure (Adaptateurs sortants)**
- `scraping/browser.py` : Interface avec Playwright
- `llm/ollama.py` : Interface avec Ollama
- `config/database.py` : Interface avec la base de donn√©es
- Responsabilit√© : Communication avec l'ext√©rieur

### 2. Inversion des d√©pendances

```python
# ‚úÖ Correct : Le service d√©pend d'une abstraction
class ScrapingService:
    def __init__(self, browser_manager: BrowserManager):
        self.browser = browser_manager  # Interface

# ‚úÖ L'adaptateur impl√©mente l'interface
class PlaywrightBrowserManager(BrowserManager):
    async def navigate(self, url: str) -> Page:
        # Impl√©mentation sp√©cifique Playwright
```

### 3. Configuration par l'ext√©rieur

```python
# Configuration centralis√©e dans config/settings.py
class Settings(BaseSettings):
    # Application
    debug: bool = False
    host: str = "0.0.0.0"
    port: int = 8000
    
    # Base de donn√©es
    database_url: str = "sqlite:///./scrapinium.db"
    
    # LLM
    ollama_host: str = "http://localhost:11434"
    ollama_model: str = "llama3.1:8b"
    
    # Performance
    max_concurrent_requests: int = 10
    request_timeout: int = 60
```

## üîÑ Flux de donn√©es

### 1. Scraping d'une URL

```mermaid
sequenceDiagram
    participant Client
    participant API
    participant ScrapingService
    participant BrowserManager
    participant ContentExtractor
    participant LLMService
    participant Database
    
    Client->>API: POST /scrape
    API->>ScrapingService: scrape_url()
    ScrapingService->>BrowserManager: navigate(url)
    BrowserManager->>BrowserManager: load_page()
    BrowserManager-->>ScrapingService: page_content
    ScrapingService->>ContentExtractor: extract_content()
    ContentExtractor-->>ScrapingService: structured_content
    ScrapingService->>LLMService: enhance_with_llm()
    LLMService-->>ScrapingService: enhanced_content
    ScrapingService->>Database: save_task()
    ScrapingService-->>API: result
    API-->>Client: JSON response
```

### 2. Gestion des t√¢ches asynchrones

```mermaid
graph LR
    A[Demande client] --> B[Cr√©ation t√¢che]
    B --> C[Queue de traitement]
    C --> D[Worker async]
    D --> E[Mise √† jour statut]
    E --> F[Notification client]
    
    D --> G[Scraping Playwright]
    D --> H[Extraction contenu]
    D --> I[Traitement LLM]
```

## üìä Mod√®les de donn√©es

### Entit√©s principales

```python
# Domain Models
@dataclass
class ScrapingTask:
    """Entit√© m√©tier pour une t√¢che de scraping."""
    task_id: str
    url: str
    status: TaskStatus
    output_format: OutputFormat
    created_at: datetime
    metadata: Dict[str, Any]

# Data Transfer Objects
class ScrapingTaskCreate(BaseModel):
    """DTO pour cr√©ation de t√¢che."""
    url: HttpUrl
    output_format: OutputFormat = OutputFormat.MARKDOWN
    use_llm: bool = True
    custom_instructions: Optional[str] = None

# Database Entities
class ScrapingTaskDB(Base):
    """Entit√© base de donn√©es."""
    __tablename__ = "scraping_tasks"
    id = Column(Integer, primary_key=True)
    task_id = Column(String(36), unique=True, index=True)
    url = Column(String(2048), nullable=False)
    status = Column(Enum(TaskStatus), default=TaskStatus.PENDING)
```

### √ânum√©rations m√©tier

```python
class TaskStatus(str, Enum):
    """Statuts possibles d'une t√¢che."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class OutputFormat(str, Enum):
    """Formats de sortie support√©s."""
    TEXT = "text"
    MARKDOWN = "markdown"
    JSON = "json"
    HTML = "html"

class LLMProvider(str, Enum):
    """Fournisseurs LLM support√©s."""
    OLLAMA = "ollama"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
```

## üîå Points d'extension

### 1. Nouveaux adaptateurs LLM

```python
# Interface commune
class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, context: str) -> str:
        pass

# Impl√©mentations
class OllamaProvider(LLMProvider):
    # Impl√©mentation Ollama locale

class OpenAIProvider(LLMProvider):
    # Impl√©mentation OpenAI API

class AnthropicProvider(LLMProvider):
    # Impl√©mentation Claude API
```

### 2. Nouveaux formats de sortie

```python
# Extracteur extensible
class ContentExtractor:
    def __init__(self):
        self.formatters = {
            OutputFormat.MARKDOWN: MarkdownFormatter(),
            OutputFormat.JSON: JSONFormatter(),
            OutputFormat.HTML: HTMLFormatter(),
            # Facile d'ajouter de nouveaux formats
        }
```

### 3. Nouveaux navigateurs

```python
# Interface navigateur
class BrowserManager(ABC):
    @abstractmethod
    async def navigate(self, url: str) -> str:
        pass

# Impl√©mentations possibles
class PlaywrightManager(BrowserManager):
    # Impl√©mentation actuelle

class SeleniumManager(BrowserManager):
    # Impl√©mentation alternative

class RequestsManager(BrowserManager):
    # Pour sites statiques
```

## üîß Configuration et d√©ploiement

### Configuration par environnement

```python
# .env.development
SCRAPINIUM_DEBUG=true
SCRAPINIUM_DATABASE_URL=sqlite:///./dev.db
SCRAPINIUM_OLLAMA_HOST=http://localhost:11434

# .env.production
SCRAPINIUM_DEBUG=false
SCRAPINIUM_DATABASE_URL=postgresql://user:pass@db:5432/scrapinium
SCRAPINIUM_OLLAMA_HOST=http://ollama:11434
```

### Docker multi-service

```yaml
# docker-compose.yml
services:
  scrapinium-app:
    build: .
    environment:
      - SCRAPINIUM_DATABASE_URL=postgresql://scrapinium:password@postgres:5432/scrapinium
      - SCRAPINIUM_OLLAMA_HOST=http://ollama:11434
    depends_on: [postgres, ollama, redis]

  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: scrapinium

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama

  redis:
    image: redis:7-alpine
```

## üìà M√©triques et observabilit√©

### Health checks distribu√©es

```python
@app.get("/health")
async def health_check():
    return {
        "api": await check_api_health(),
        "database": await check_database_health(),
        "ollama": await check_ollama_health(),
        "redis": await check_redis_health(),
    }
```

### Logging structur√©

```python
# Logs JSON pour agr√©gation
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "INFO",
  "service": "scrapinium.scraping",
  "task_id": "abc-123",
  "url": "https://example.com",
  "duration_ms": 1500,
  "status": "completed"
}
```

## üéØ Avantages de cette architecture

### ‚úÖ Testabilit√© maximale
- Tests unitaires isol√©s par couche
- Mocks faciles avec interfaces
- Tests d'int√©gration cibl√©s

### ‚úÖ √âvolutivit√© technique
- Ajout de nouveaux LLMs sans impact
- Changement de base de donn√©es transparent
- Support multi-navigateurs

### ‚úÖ Maintenance simplifi√©e
- Code d√©coupl√© et modulaire
- Responsabilit√©s claires
- Refactoring s√©curis√©

### ‚úÖ Performance optimis√©e
- Traitement asynchrone natif
- Cache multi-niveaux
- Pools de connexions

## üöÄ Prochaines √©volutions

### v0.2.0 - Interface compl√®te
- Interface Reflex compl√®te
- Dashboard de monitoring
- Gestion d'utilisateurs

### v0.3.0 - Distribution
- Workers Celery distribu√©s
- Load balancing intelligent
- M√©triques Prometheus

### v1.0.0 - Enterprise
- Multi-tenancy
- Authentification SSO
- Audit et compliance

---

Cette architecture garantit que Scrapinium peut √©voluer de mani√®re contr√¥l√©e tout en maintenant la qualit√© et les performances. Elle respecte les principes SOLID et facilite les tests automatis√©s.