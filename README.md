# Scrapinium üï∏Ô∏èü§ñ

**Scrapinium** est une solution de scraping web intelligent qui utilise des mod√®les de langage (LLMs) pour extraire et structurer automatiquement le contenu des sites web. Contrairement aux scrapers traditionnels, Scrapinium comprend le contenu et le transforme en documents structur√©s selon vos besoins.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Playwright](https://img.shields.io/badge/Playwright-1.40+-purple.svg)](https://playwright.dev)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## üéØ Vision

Transformer le web scraping d'une t√¢che technique r√©p√©titive en un processus intelligent et automatis√©, permettant aux utilisateurs de se concentrer sur l'analyse plut√¥t que sur l'extraction des donn√©es.

## ‚ú® Fonctionnalit√©s principales

### üîß Scraping intelligent
- **Navigation JavaScript** : Support complet des SPAs et sites dynamiques avec Playwright
- **Pool de navigateurs optimis√©** : 3-5x am√©lioration des performances de concurrence
- **Extraction adaptative** : Algorithmes intelligents pour identifier le contenu principal avec Readability
- **Formats multiples** : Export en Markdown, Text, JSON, HTML
- **Gestion robuste des erreurs** : Recovery automatique et retry intelligent

### üß† Intelligence artificielle
- **Pipeline ML int√©gr√©** : Classification, d√©tection anti-bot, analyse s√©mantique
- **Structuration automatique** : Ollama local pour analyser et organiser le contenu
- **Support LLM local** : Llama 3.1 8B par d√©faut, extensible √† d'autres mod√®les
- **Classification intelligente** : 7 types de contenu avec √©valuation qualit√©
- **D√©tection anti-bot** : Cloudflare, reCAPTCHA, strat√©gies d'√©vasion automatiques
- **Analyse s√©mantique** : Mots-cl√©s, sentiment, topics, m√©triques de lisibilit√©
- **Instructions personnalis√©es** : Prompts configurables pour chaque t√¢che
- **Cache ML intelligent** : Analyse rapide avec TTL et auto-nettoyage

### üèóÔ∏è Architecture moderne
- **API REST compl√®te** : Interface FastAPI avec endpoints document√©s
- **Interface web HTML/JS** : Interface moderne avec Tailwind CSS et Alpine.js
- **Base de donn√©es SQLAlchemy** : Support SQLite et PostgreSQL
- **Docker ready** : D√©ploiement simplifi√© avec containerisation

### üöÄ Performances optimis√©es
- **Cache multi-niveau** : Redis + m√©moire avec hit rate de 91%+ et 8500+ ops/sec
- **Pool de navigateurs** : Gestion intelligente de 3-5 instances Chromium
- **Streaming de contenu** : Traitement par chunks pour √©conomiser la m√©moire
- **Compression adaptative** : GZIP/LZ4/Brotli avec 95%+ d'√©conomie d'espace

### üîí Robustesse et monitoring
- **Surveillance m√©moire** : Monitoring temps r√©el avec seuils automatiques
- **Nettoyage automatique** : Garbage collection et lib√©ration de ressources
- **Health checks avanc√©s** : Monitoring syst√®me, cache, pool navigateurs
- **Gestion des t√¢ches** : Queue avec statuts (pending, running, completed, failed)
- **APIs de maintenance** : GC forc√©, optimisation m√©moire, nettoyage ressources

## üöÄ Installation rapide

### Pr√©requis
- Python 3.11+
- Poetry (gestionnaire de d√©pendances)
- Docker & Docker Compose (optionnel)
- Ollama (pour les fonctionnalit√©s LLM)

### Installation locale

```bash
# Cloner le repository
git clone https://github.com/votre-username/scrapinium.git
cd scrapinium

# Installer les d√©pendances
poetry install

# Installer Playwright
poetry run playwright install chromium

# Configuration initiale
cp .env.example .env
# √âditer .env avec vos param√®tres

# Lancer l'application FastAPI principale
poetry run python src/scrapinium/api/app.py
# ou
make dev

# Lancer un exemple sp√©cifique
poetry run python examples/fastapi/app.py
poetry run streamlit run examples/streamlit/streamlit_app.py
```

### Installation avec Docker

```bash
# Lancer avec Docker Compose
docker-compose up --build

# Ou en mode d√©veloppement
make docker-dev

# Configurer Ollama
make setup-ollama
```

## üìä Utilisation

### API REST

L'API est accessible sur `http://localhost:8000` avec une documentation interactive √† `/docs`.

#### Endpoints principaux

**Health Check**
```bash
curl http://localhost:8000/health
```

**D√©marrer un scraping**
```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "output_format": "markdown",
    "use_llm": true,
    "custom_instructions": "Extraire uniquement les articles principaux"
  }'
```

**Suivre le progr√®s**
```bash
curl "http://localhost:8000/scrape/{task_id}"
```

**R√©cup√©rer le r√©sultat**
```bash
curl "http://localhost:8000/scrape/{task_id}/result"
```

**Lister toutes les t√¢ches**
```bash
curl "http://localhost:8000/tasks?limit=10"
```

**Statistiques**
```bash
curl "http://localhost:8000/stats"
```

**Statistiques d√©taill√©es**
```bash
# Statistiques du cache multi-niveau
curl "http://localhost:8000/stats/cache"

# Statistiques du pool de navigateurs
curl "http://localhost:8000/stats/browser"

# Statistiques m√©moire
curl "http://localhost:8000/stats/memory"

# Statistiques de nettoyage
curl "http://localhost:8000/stats/cleanup"
```

**Maintenance syst√®me**
```bash
# Vider le cache
curl -X DELETE "http://localhost:8000/cache"

# Forcer garbage collection
curl -X POST "http://localhost:8000/maintenance/gc"

# Optimiser la m√©moire
curl -X POST "http://localhost:8000/maintenance/optimize"

# Nettoyer les ressources
curl -X POST "http://localhost:8000/maintenance/cleanup"
```

### Interface utilisateur

Une interface web moderne est disponible sur `http://localhost:8000` avec :
- **Dashboard temps r√©el** : Statistiques du syst√®me et monitoring
- **Interface de scraping** : Formulaire intuitif pour cr√©er des t√¢ches
- **Visualisation des r√©sultats** : Modal pour afficher le contenu extrait
- **Gestion des t√¢ches** : Liste et statut de toutes les t√¢ches
- **Th√®me sombre √©l√©gant** : Design moderne avec glassmorphism

### Exemples d'utilisation

Scrapinium propose plusieurs interfaces :

#### API REST (Production)
```bash
# Utiliser l'API principale
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "output_format": "markdown"}'
```

#### Interface Streamlit (Prototypage)
```bash
# Interface interactive pour tests et d√©mos
streamlit run examples/streamlit/streamlit_app.py
```

#### Utilisation programmatique
```python
from scrapinium.scraping import scraping_service
from scrapinium.models import ScrapingTaskCreate

# Configuration de la t√¢che
task_data = ScrapingTaskCreate(
    url="https://example.com",
    output_format="markdown",
    use_llm=True,
    custom_instructions="Extraire les informations principales"
)

# Ex√©cution du scraping
result = await scraping_service.scrape_url(
    task_data=task_data,
    task_id="test-task"
)

print(f"R√©sultat: {result['structured_content']}")
```

## üèóÔ∏è Architecture

Scrapinium suit une architecture hexagonale moderne avec s√©paration claire des responsabilit√©s :

```
src/scrapinium/
‚îú‚îÄ‚îÄ api/              # Interface API (FastAPI)
‚îú‚îÄ‚îÄ cache/            # Syst√®me de cache multi-niveau
‚îú‚îÄ‚îÄ scraping/         # C≈ìur du scraping optimis√©
‚îú‚îÄ‚îÄ ml/               # Pipeline Machine Learning
‚îú‚îÄ‚îÄ llm/              # Int√©gration LLM
‚îú‚îÄ‚îÄ utils/            # Utilitaires d'optimisation
‚îú‚îÄ‚îÄ models/           # Mod√®les de donn√©es
‚îî‚îÄ‚îÄ config/           # Configuration

examples/             # Applications d'exemple
‚îú‚îÄ‚îÄ fastapi/          # Exemple API REST avanc√©e
‚îú‚îÄ‚îÄ streamlit/        # Interface interactive
‚îî‚îÄ‚îÄ reflex/           # Interface Python-native (legacy)

requirements/         # D√©pendances organis√©es
‚îú‚îÄ‚îÄ base.txt          # D√©pendances principales
‚îú‚îÄ‚îÄ dev.txt           # D√©veloppement
‚îú‚îÄ‚îÄ prod.txt          # Production
‚îî‚îÄ‚îÄ ml.txt            # Machine Learning

docs/                 # Documentation compl√®te
tests/                # Tests unitaires et d'int√©gration
.tmp/                 # Fichiers temporaires (logs, cache, db)
```

### Technologies utilis√©es

**Backend Core :**
- **FastAPI 0.104+** : API REST moderne et performante
- **Playwright 1.40+** : Automatisation de navigateur avec support JavaScript
- **SQLAlchemy 2.0+** : ORM Python avec support async
- **Pydantic 2.5+** : Validation de donn√©es avec types Python
- **Redis 5.0+** : Cache et queue de t√¢ches

**Scraping & Extraction :**
- **BeautifulSoup4** : Parsing HTML robuste
- **Readability-lxml** : Extraction de contenu principal
- **html2text** : Conversion HTML vers Markdown

**Intelligence :**
- **Ollama** : LLMs locaux (Llama 3.1 8B par d√©faut)
- **HTTPX** : Client HTTP async pour communication avec Ollama

**Frontend :**
- **Interface HTML/JS moderne** : Tailwind CSS + Alpine.js
- **Th√®me sombre √©l√©gant** : Glassmorphism et animations fluides
- **Reflex 0.4+** : Framework web Python (legacy/optionnel)

**DevOps & Outils :**
- **Docker & Docker Compose** : Containerisation
- **Poetry** : Gestion des d√©pendances Python
- **Ruff** : Linting et formatage de code ultra-rapide
- **pytest** : Framework de tests
- **Uvicorn** : Serveur ASGI haute performance

## üîß Configuration

### Variables d'environnement

```bash
# Application
SCRAPINIUM_DEBUG=false
SCRAPINIUM_HOST=0.0.0.0
SCRAPINIUM_PORT=8000

# Base de donn√©es
SCRAPINIUM_DATABASE_URL=sqlite:///./scrapinium.db
# ou PostgreSQL: postgresql://user:password@localhost:5432/scrapinium

# Redis (cache et queues)
SCRAPINIUM_REDIS_URL=redis://localhost:6379

# LLM Ollama
SCRAPINIUM_OLLAMA_HOST=http://localhost:11434
SCRAPINIUM_OLLAMA_MODEL=llama3.1:8b

# S√©curit√©
SCRAPINIUM_SECRET_KEY=your-super-secret-production-key

# Performance
SCRAPINIUM_MAX_CONCURRENT_REQUESTS=10
SCRAPINIUM_REQUEST_TIMEOUT=60

# Logging
SCRAPINIUM_LOG_LEVEL=INFO
SCRAPINIUM_LOG_FORMAT=json
```

### Mod√®les de donn√©es

```python
# Formats de sortie support√©s
class OutputFormat(str, Enum):
    TEXT = "text"
    MARKDOWN = "markdown" 
    JSON = "json"
    HTML = "html"

# Statuts des t√¢ches
class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
```

## üìà Monitoring et observabilit√©

### Health Checks
```bash
# V√©rifier l'√©tat complet du syst√®me
curl http://localhost:8000/health

# R√©ponse exemple:
{
  "api": "healthy",
  "database": "healthy", 
  "ollama": "unhealthy"  # Si Ollama n'est pas d√©marr√©
}
```

### M√©triques et statistiques avanc√©es
```bash
# Statistiques globales avec cache et pool
curl http://localhost:8000/stats

# R√©ponse exemple:
{
  "total_tasks": 15,
  "active_tasks": 2,
  "completed_tasks": 10,
  "failed_tasks": 3,
  "success_rate": 66.7,
  "ollama_status": "connected",
  "browser_pool": {
    "total_browsers": 3,
    "active_browsers": 1,
    "avg_wait_time_ms": 15.2,
    "peak_usage": 3
  }
}

# Cache multi-niveau
curl http://localhost:8000/stats/cache
{
  "hit_rate": 91.67,
  "total_requests": 24,
  "memory_cache": {"entry_count": 11, "utilization": 55},
  "redis_cache": {"connected": true, "entry_count": 11}
}

# M√©moire syst√®me  
curl http://localhost:8000/stats/memory
{
  "current_usage": {"process_memory_mb": 245.8},
  "statistics": {"peak_usage_mb": 289.1, "usage_trend": "stable"},
  "thresholds": {"warning_reached": false, "critical_reached": false}
}
```

### Logs structur√©s
Les logs sont format√©s en JSON pour faciliter l'agr√©gation :
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "logger": "scrapinium.scraping.service",
  "message": "T√¢che de scraping d√©marr√©e",
  "task_id": "123e4567-e89b-12d3-a456-426614174000",
  "url": "https://example.com"
}
```

## üß™ Tests et validation

```bash
# Tests unitaires complets
poetry run pytest tests/ -v

# Tests d'int√©gration
poetry run pytest tests/integration/ -v

# Tests avec coverage
poetry run pytest --cov=scrapinium tests/

# Linting et formatage
poetry run ruff check src/
poetry run ruff format src/

# Tests des exemples
python examples/fastapi/app.py &
curl http://localhost:8000/health
```

### Tests fonctionnels

```bash
# Test complet via API
make test

# Test de scraping simple via exemple FastAPI
curl -X POST "http://localhost:8001/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "output_format": "markdown", "use_llm": false}'

# Test via interface Streamlit
streamlit run examples/streamlit/streamlit_app.py
```

## üìö Commandes disponibles

Le projet inclut un Makefile avec toutes les commandes utiles :

```bash
# Installation et setup
make install          # Installe les d√©pendances
make setup-ollama     # Configure Ollama avec le mod√®le par d√©faut

# D√©veloppement
make dev              # Lance l'application en mode d√©veloppement
make test             # Lance les tests
make lint             # V√©rifie le code avec ruff
make format           # Formate le code

# Docker
make docker-dev       # Lance l'environnement avec Docker
make docker-prod      # Lance en mode production
make docker-down      # Arr√™te les conteneurs

# Utilitaires
make clean            # Nettoie les fichiers temporaires
make health           # V√©rifie la sant√© de l'application
make backup-db        # Sauvegarde la base de donn√©es
make shell            # Lance un shell Python avec le contexte
```

## üõ£Ô∏è Roadmap

### v0.2.0 - Optimisations avanc√©es ‚úÖ TERMIN√â
- [x] API REST fonctionnelle
- [x] Service de scraping robuste
- [x] Int√©gration LLM locale
- [x] Interface web HTML/JS moderne
- [x] Pool de navigateurs optimis√© (3-5x performance)
- [x] Cache multi-niveau Redis + m√©moire (91%+ hit rate)
- [x] Surveillance m√©moire temps r√©el
- [x] Compression et streaming optimis√©s
- [x] Nettoyage automatique des ressources

### ‚úÖ v0.3.0 - ML Integration (Juin 2025) - TERMIN√â
- [x] **Pipeline ML complet** avec classification, anti-bot, analyse s√©mantique
- [x] **7 nouveaux endpoints ML** dans l'API REST
- [x] **Cache ML intelligent** avec parall√©lisation pour performances optimales
- [x] **Analyse automatique** int√©gr√©e dans le workflow de scraping
- [x] **Tests complets** avec 19 tests unitaires ML

### v0.4.0 - Features avanc√©es (En cours)
- [ ] Support multi-LLM (OpenAI, Anthropic, Gemini)
- [ ] WebSockets pour updates temps r√©el
- [ ] Interface web ML avec dashboards
- [ ] Scraping distribu√© avec Celery
- [ ] Rate limiting intelligent par domaine
- [ ] Export vers multiple formats (PDF, CSV, XML)

### v1.0.0 - Production ready
- [ ] Authentification et autorisation JWT
- [ ] API webhooks pour notifications
- [ ] M√©triques Prometheus/Grafana
- [ ] D√©ploiement cloud (AWS, GCP, Azure)
- [ ] Interface admin compl√®te

## ü§ù Contribution

Les contributions sont les bienvenues ! Le projet suit les principes DRY et KISS.

### Comment contribuer
1. Fork le projet
2. Cr√©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Suivre les conventions de code (ruff, tests)
4. Commit vos changements (`git commit -m 'Add AmazingFeature'`)
5. Push vers la branche (`git push origin feature/AmazingFeature`)
6. Ouvrir une Pull Request

### Standards de d√©veloppement
- Code Python 3.11+ avec type hints
- Tests unitaires obligatoires
- Documentation des fonctions publiques
- Respect des conventions PEP 8 (via ruff)

## üìù License

Ce projet est sous licence MIT. Voir [LICENSE](./LICENSE) pour plus de d√©tails.

## üôè Remerciements

- [Playwright](https://playwright.dev/) pour l'automatisation de navigateur moderne
- [FastAPI](https://fastapi.tiangolo.com/) pour le framework web performant
- [Ollama](https://ollama.ai/) pour les LLMs locaux accessibles
- [Reflex](https://reflex.dev/) pour l'interface utilisateur Python-native
- [Readability](https://github.com/buriy/python-readability) pour l'extraction de contenu

## üìû Support

Pour toute question ou probl√®me :

- üêõ **Issues** : [GitHub Issues](https://github.com/votre-username/scrapinium/issues)
- üìñ **Documentation** : Consultez les fichiers dans `/docs`
- üí¨ **Discussions** : Utilisez les GitHub Discussions

## üöÄ Statut du projet

**Version actuelle** : 0.3.0  
**Statut** : ‚úÖ Production Ready - API compl√®te, ML pipeline int√©gr√©, structure professionnelle  
**Derni√®re mise √† jour** : Juin 2025

---

‚≠ê Si ce projet vous aide, n'h√©sitez pas √† lui donner une √©toile sur GitHub !