# Scrapinium ğŸ•¸ï¸ğŸ¤–

**Scrapinium** est une solution de scraping web intelligent qui utilise des modÃ¨les de langage (LLMs) pour extraire et structurer automatiquement le contenu des sites web. Contrairement aux scrapers traditionnels, Scrapinium comprend le contenu et le transforme en documents structurÃ©s selon vos besoins.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Playwright](https://img.shields.io/badge/Playwright-1.40+-purple.svg)](https://playwright.dev)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ¯ Vision

Transformer le web scraping d'une tÃ¢che technique rÃ©pÃ©titive en un processus intelligent et automatisÃ©, permettant aux utilisateurs de se concentrer sur l'analyse plutÃ´t que sur l'extraction des donnÃ©es.

## âœ¨ FonctionnalitÃ©s principales

### ğŸ”§ Scraping intelligent
- **Navigation JavaScript** : Support complet des SPAs et sites dynamiques avec Playwright
- **Pool de navigateurs optimisÃ©** : 3-5x amÃ©lioration des performances de concurrence
- **Extraction adaptative** : Algorithmes intelligents pour identifier le contenu principal avec Readability
- **Formats multiples** : Export en Markdown, Text, JSON, HTML
- **Gestion robuste des erreurs** : Recovery automatique et retry intelligent

### ğŸ§  Intelligence artificielle
- **Structuration automatique** : Ollama local pour analyser et organiser le contenu
- **Support LLM local** : Llama 3.1 8B par dÃ©faut, extensible Ã  d'autres modÃ¨les
- **Classification intelligente** : Identification automatique du type de contenu
- **Instructions personnalisÃ©es** : Prompts configurables pour chaque tÃ¢che
- **Cache intelligent LLM** : Mise en cache des rÃ©sultats pour Ã©viter les rÃ©pÃ©titions

### ğŸ—ï¸ Architecture moderne
- **API REST complÃ¨te** : Interface FastAPI avec endpoints documentÃ©s
- **Interface web HTML/JS** : Interface moderne avec Tailwind CSS et Alpine.js
- **Base de donnÃ©es SQLAlchemy** : Support SQLite et PostgreSQL
- **Docker ready** : DÃ©ploiement simplifiÃ© avec containerisation

### ğŸš€ Performances optimisÃ©es
- **Cache multi-niveau** : Redis + mÃ©moire avec hit rate de 91%+ et 8500+ ops/sec
- **Pool de navigateurs** : Gestion intelligente de 3-5 instances Chromium
- **Streaming de contenu** : Traitement par chunks pour Ã©conomiser la mÃ©moire
- **Compression adaptative** : GZIP/LZ4/Brotli avec 95%+ d'Ã©conomie d'espace

### ğŸ”’ Robustesse et monitoring
- **Surveillance mÃ©moire** : Monitoring temps rÃ©el avec seuils automatiques
- **Nettoyage automatique** : Garbage collection et libÃ©ration de ressources
- **Health checks avancÃ©s** : Monitoring systÃ¨me, cache, pool navigateurs
- **Gestion des tÃ¢ches** : Queue avec statuts (pending, running, completed, failed)
- **APIs de maintenance** : GC forcÃ©, optimisation mÃ©moire, nettoyage ressources

## ğŸš€ Installation rapide

### PrÃ©requis
- Python 3.11+
- Poetry (gestionnaire de dÃ©pendances)
- Docker & Docker Compose (optionnel)
- Ollama (pour les fonctionnalitÃ©s LLM)

### Installation locale

```bash
# Cloner le repository
git clone https://github.com/votre-username/scrapinium.git
cd scrapinium

# Installer les dÃ©pendances
poetry install

# Installer Playwright
poetry run playwright install chromium

# Configuration initiale
cp .env.example .env
# Ã‰diter .env avec vos paramÃ¨tres

# Lancer l'application
poetry run python main.py
# ou
make dev
```

### Installation avec Docker

```bash
# Lancer avec Docker Compose
docker-compose up --build

# Ou en mode dÃ©veloppement
make docker-dev

# Configurer Ollama
make setup-ollama
```

## ğŸ“Š Utilisation

### API REST

L'API est accessible sur `http://localhost:8000` avec une documentation interactive Ã  `/docs`.

#### Endpoints principaux

**Health Check**
```bash
curl http://localhost:8000/health
```

**DÃ©marrer un scraping**
```bash
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "output_format": "markdown",
    "use_llm": true,
    "custom_instructions": "Extraire uniquement les articles principaux"
  }'
```

**Suivre le progrÃ¨s**
```bash
curl "http://localhost:8000/scrape/{task_id}"
```

**RÃ©cupÃ©rer le rÃ©sultat**
```bash
curl "http://localhost:8000/scrape/{task_id}/result"
```

**Lister toutes les tÃ¢ches**
```bash
curl "http://localhost:8000/tasks?limit=10"
```

**Statistiques**
```bash
curl "http://localhost:8000/stats"
```

**Statistiques dÃ©taillÃ©es**
```bash
# Statistiques du cache multi-niveau
curl "http://localhost:8000/stats/cache"

# Statistiques du pool de navigateurs
curl "http://localhost:8000/stats/browser"

# Statistiques mÃ©moire
curl "http://localhost:8000/stats/memory"

# Statistiques de nettoyage
curl "http://localhost:8000/stats/cleanup"
```

**Maintenance systÃ¨me**
```bash
# Vider le cache
curl -X DELETE "http://localhost:8000/cache"

# Forcer garbage collection
curl -X POST "http://localhost:8000/maintenance/gc"

# Optimiser la mÃ©moire
curl -X POST "http://localhost:8000/maintenance/optimize"

# Nettoyer les ressources
curl -X POST "http://localhost:8000/maintenance/cleanup"
```

### Interface utilisateur

Une interface web moderne est disponible sur `http://localhost:8000` avec :
- **Dashboard temps rÃ©el** : Statistiques du systÃ¨me et monitoring
- **Interface de scraping** : Formulaire intuitif pour crÃ©er des tÃ¢ches
- **Visualisation des rÃ©sultats** : Modal pour afficher le contenu extrait
- **Gestion des tÃ¢ches** : Liste et statut de toutes les tÃ¢ches
- **ThÃ¨me sombre Ã©lÃ©gant** : Design moderne avec glassmorphism

### Utilisation programmatique

```python
from scrapinium.scraping import scraping_service
from scrapinium.models import ScrapingTaskCreate

# Configuration de la tÃ¢che
task_data = ScrapingTaskCreate(
    url="https://example.com",
    output_format="markdown",
    use_llm=True,
    custom_instructions="Extraire les informations principales"
)

# Callback pour suivre le progrÃ¨s
async def progress_callback(task_id, progress, message):
    print(f"TÃ¢che {task_id}: {progress}% - {message}")

# ExÃ©cution du scraping
result = await scraping_service.scrape_url(
    task_data=task_data,
    task_id="test-task",
    progress_callback=progress_callback
)

print(f"RÃ©sultat: {result['structured_content']}")
```

## ğŸ—ï¸ Architecture

Scrapinium suit une architecture hexagonale moderne avec sÃ©paration claire des responsabilitÃ©s :

```
src/scrapinium/
â”œâ”€â”€ api/              # Interface API (FastAPI)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ app.py        # Application principale et routes avancÃ©es
â”œâ”€â”€ cache/            # SystÃ¨me de cache multi-niveau
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ manager.py    # Gestionnaire Redis + mÃ©moire
â”‚   â”œâ”€â”€ models.py     # ModÃ¨les de cache
â”‚   â””â”€â”€ strategies.py # StratÃ©gies d'Ã©viction (LRU, TTL)
â”œâ”€â”€ scraping/         # CÅ“ur du scraping optimisÃ©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ browser.py    # Pool de navigateurs Playwright
â”‚   â”œâ”€â”€ extractor.py  # Extraction de contenu
â”‚   â””â”€â”€ service.py    # Service principal avec cache
â”œâ”€â”€ llm/              # IntÃ©gration LLM
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ollama.py     # Client Ollama avec cache
â”œâ”€â”€ utils/            # Utilitaires d'optimisation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ memory.py     # Monitoring mÃ©moire avancÃ©
â”‚   â”œâ”€â”€ streaming.py  # Traitement streaming
â”‚   â”œâ”€â”€ compression.py # Compression multi-algo
â”‚   â”œâ”€â”€ cleanup.py    # Nettoyage automatique
â”‚   â”œâ”€â”€ helpers.py    # Fonctions utilitaires
â”‚   â””â”€â”€ validators.py # Validateurs
â”œâ”€â”€ models/           # ModÃ¨les de donnÃ©es
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py   # ModÃ¨les SQLAlchemy
â”‚   â”œâ”€â”€ enums.py      # Ã‰numÃ©rations
â”‚   â””â”€â”€ schemas.py    # SchÃ©mas Pydantic
â”œâ”€â”€ config/           # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py   # Configuration DB
â”‚   â””â”€â”€ settings.py   # Settings Pydantic
â””â”€â”€ ui/               # Interface utilisateur (legacy)
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ app.py        # App Reflex (dÃ©prÃ©ciÃ©)
    â”œâ”€â”€ components/   # Composants rÃ©utilisables
    â””â”€â”€ styles.py     # ThÃ¨me sombre et styles
```

### Technologies utilisÃ©es

**Backend Core :**
- **FastAPI 0.104+** : API REST moderne et performante
- **Playwright 1.40+** : Automatisation de navigateur avec support JavaScript
- **SQLAlchemy 2.0+** : ORM Python avec support async
- **Pydantic 2.5+** : Validation de donnÃ©es avec types Python
- **Redis 5.0+** : Cache et queue de tÃ¢ches

**Scraping & Extraction :**
- **BeautifulSoup4** : Parsing HTML robuste
- **Readability-lxml** : Extraction de contenu principal
- **html2text** : Conversion HTML vers Markdown

**Intelligence :**
- **Ollama** : LLMs locaux (Llama 3.1 8B par dÃ©faut)
- **HTTPX** : Client HTTP async pour communication avec Ollama

**Frontend :**
- **Interface HTML/JS moderne** : Tailwind CSS + Alpine.js
- **ThÃ¨me sombre Ã©lÃ©gant** : Glassmorphism et animations fluides
- **Reflex 0.4+** : Framework web Python (legacy/optionnel)

**DevOps & Outils :**
- **Docker & Docker Compose** : Containerisation
- **Poetry** : Gestion des dÃ©pendances Python
- **Ruff** : Linting et formatage de code ultra-rapide
- **pytest** : Framework de tests
- **Uvicorn** : Serveur ASGI haute performance

## ğŸ”§ Configuration

### Variables d'environnement

```bash
# Application
SCRAPINIUM_DEBUG=false
SCRAPINIUM_HOST=0.0.0.0
SCRAPINIUM_PORT=8000

# Base de donnÃ©es
SCRAPINIUM_DATABASE_URL=sqlite:///./scrapinium.db
# ou PostgreSQL: postgresql://user:password@localhost:5432/scrapinium

# Redis (cache et queues)
SCRAPINIUM_REDIS_URL=redis://localhost:6379

# LLM Ollama
SCRAPINIUM_OLLAMA_HOST=http://localhost:11434
SCRAPINIUM_OLLAMA_MODEL=llama3.1:8b

# SÃ©curitÃ©
SCRAPINIUM_SECRET_KEY=your-super-secret-production-key

# Performance
SCRAPINIUM_MAX_CONCURRENT_REQUESTS=10
SCRAPINIUM_REQUEST_TIMEOUT=60

# Logging
SCRAPINIUM_LOG_LEVEL=INFO
SCRAPINIUM_LOG_FORMAT=json
```

### ModÃ¨les de donnÃ©es

```python
# Formats de sortie supportÃ©s
class OutputFormat(str, Enum):
    TEXT = "text"
    MARKDOWN = "markdown" 
    JSON = "json"
    HTML = "html"

# Statuts des tÃ¢ches
class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
```

## ğŸ“ˆ Monitoring et observabilitÃ©

### Health Checks
```bash
# VÃ©rifier l'Ã©tat complet du systÃ¨me
curl http://localhost:8000/health

# RÃ©ponse exemple:
{
  "api": "healthy",
  "database": "healthy", 
  "ollama": "unhealthy"  # Si Ollama n'est pas dÃ©marrÃ©
}
```

### MÃ©triques et statistiques avancÃ©es
```bash
# Statistiques globales avec cache et pool
curl http://localhost:8000/stats

# RÃ©ponse exemple:
{
  "total_tasks": 15,
  "active_tasks": 2,
  "completed_tasks": 10,
  "failed_tasks": 3,
  "success_rate": 66.7,
  "ollama_status": "connected",
  "browser_pool": {
    "total_browsers": 3,
    "active_browsers": 1,
    "avg_wait_time_ms": 15.2,
    "peak_usage": 3
  }
}

# Cache multi-niveau
curl http://localhost:8000/stats/cache
{
  "hit_rate": 91.67,
  "total_requests": 24,
  "memory_cache": {"entry_count": 11, "utilization": 55},
  "redis_cache": {"connected": true, "entry_count": 11}
}

# MÃ©moire systÃ¨me  
curl http://localhost:8000/stats/memory
{
  "current_usage": {"process_memory_mb": 245.8},
  "statistics": {"peak_usage_mb": 289.1, "usage_trend": "stable"},
  "thresholds": {"warning_reached": false, "critical_reached": false}
}
```

### Logs structurÃ©s
Les logs sont formatÃ©s en JSON pour faciliter l'agrÃ©gation :
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "logger": "scrapinium.scraping.service",
  "message": "TÃ¢che de scraping dÃ©marrÃ©e",
  "task_id": "123e4567-e89b-12d3-a456-426614174000",
  "url": "https://example.com"
}
```

## ğŸ§ª Tests et validation

```bash
# Tests unitaires complets
poetry run pytest tests/ -v

# Tests d'intÃ©gration
poetry run pytest tests/integration/ -v

# Tests avec coverage
poetry run pytest --cov=scrapinium tests/

# Linting et formatage
poetry run ruff check src/
poetry run ruff format src/

# Tests manuels des modules
poetry run python -c "from scrapinium.config import settings; print('âœ“ Config OK')"
poetry run python -c "from scrapinium.api.app import app; print('âœ“ API OK')"
```

### Tests fonctionnels

```bash
# Test complet via API
make test

# Test de scraping simple
curl -X POST "http://localhost:8000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "output_format": "markdown", "use_llm": false}'
```

## ğŸ“š Commandes disponibles

Le projet inclut un Makefile avec toutes les commandes utiles :

```bash
# Installation et setup
make install          # Installe les dÃ©pendances
make setup-ollama     # Configure Ollama avec le modÃ¨le par dÃ©faut

# DÃ©veloppement
make dev              # Lance l'application en mode dÃ©veloppement
make test             # Lance les tests
make lint             # VÃ©rifie le code avec ruff
make format           # Formate le code

# Docker
make docker-dev       # Lance l'environnement avec Docker
make docker-prod      # Lance en mode production
make docker-down      # ArrÃªte les conteneurs

# Utilitaires
make clean            # Nettoie les fichiers temporaires
make health           # VÃ©rifie la santÃ© de l'application
make backup-db        # Sauvegarde la base de donnÃ©es
make shell            # Lance un shell Python avec le contexte
```

## ğŸ›£ï¸ Roadmap

### v0.2.0 - Optimisations avancÃ©es âœ… TERMINÃ‰
- [x] API REST fonctionnelle
- [x] Service de scraping robuste
- [x] IntÃ©gration LLM locale
- [x] Interface web HTML/JS moderne
- [x] Pool de navigateurs optimisÃ© (3-5x performance)
- [x] Cache multi-niveau Redis + mÃ©moire (91%+ hit rate)
- [x] Surveillance mÃ©moire temps rÃ©el
- [x] Compression et streaming optimisÃ©s
- [x] Nettoyage automatique des ressources

### v0.3.0 - Features avancÃ©es (En cours)
- [ ] Support multi-LLM (OpenAI, Anthropic, Gemini)
- [ ] WebSockets pour updates temps rÃ©el
- [ ] Scraping distribuÃ© avec Celery
- [ ] Rate limiting intelligent par domaine
- [ ] Export vers multiple formats (PDF, CSV, XML)

### v1.0.0 - Production ready
- [ ] Authentification et autorisation JWT
- [ ] API webhooks pour notifications
- [ ] MÃ©triques Prometheus/Grafana
- [ ] DÃ©ploiement cloud (AWS, GCP, Azure)
- [ ] Interface admin complÃ¨te

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Le projet suit les principes DRY et KISS.

### Comment contribuer
1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Suivre les conventions de code (ruff, tests)
4. Commit vos changements (`git commit -m 'Add AmazingFeature'`)
5. Push vers la branche (`git push origin feature/AmazingFeature`)
6. Ouvrir une Pull Request

### Standards de dÃ©veloppement
- Code Python 3.11+ avec type hints
- Tests unitaires obligatoires
- Documentation des fonctions publiques
- Respect des conventions PEP 8 (via ruff)

## ğŸ“ License

Ce projet est sous licence MIT. Voir [LICENSE](./LICENSE) pour plus de dÃ©tails.

## ğŸ™ Remerciements

- [Playwright](https://playwright.dev/) pour l'automatisation de navigateur moderne
- [FastAPI](https://fastapi.tiangolo.com/) pour le framework web performant
- [Ollama](https://ollama.ai/) pour les LLMs locaux accessibles
- [Reflex](https://reflex.dev/) pour l'interface utilisateur Python-native
- [Readability](https://github.com/buriy/python-readability) pour l'extraction de contenu

## ğŸ“ Support

Pour toute question ou problÃ¨me :

- ğŸ› **Issues** : [GitHub Issues](https://github.com/votre-username/scrapinium/issues)
- ğŸ“– **Documentation** : Consultez les fichiers dans `/docs`
- ğŸ’¬ **Discussions** : Utilisez les GitHub Discussions

## ğŸš€ Statut du projet

**Version actuelle** : 0.1.0  
**Statut** : âœ… Fonctionnel - API complÃ¨te, scraping opÃ©rationnel, LLM intÃ©grÃ©  
**DerniÃ¨re mise Ã  jour** : Janvier 2025

---

â­ Si ce projet vous aide, n'hÃ©sitez pas Ã  lui donner une Ã©toile sur GitHub !